# Отчёт по задаче: Максимальное значение элементов матрицы

**Национальный исследовательский Нижегородский государственный университет им. Н.И. Лобачевского**  
Институт информационных технологий, математики и механики

| | |
|------|------|
| **Направление подготовки** | Прикладная математика и информатика |
| **Вариант задания** | №13 |
| **Студент** | Болтенков С.С. (группа 3823Б1ПМоп3) |
| **Преподаватель** | доцент Сысоев А.В. |
| **Год выполнения** | 2025 |

## Введение
Разработан алгоритм поиска максимального элемента матрицы с реализацией в последовательной и параллельной версиях (MPI). Проведён сравнительный анализ производительности. Параллельная реализация направлена на сокращение времени вычислений за счёт распределения данных по процессам.


## Постановка задачи
Требовалось для заданной матрицы, представленной в виде одномерного массива (вектор `std::vector<double>`), найти максимальное значение среди всех её элементов.  

Предполагается, что матрица имеет строго заданную размерность **n × m**, где:  
- *n* — количество строк матрицы,  
- *m* — количество столбцов матрицы.  

Элементы матрицы хранятся **по строкам**, что означает, что элемент с индексами *(i, j)* располагается в массиве под линейным индексом `i * n + j`.

Требуется:  
1. Реализовать последовательный алгоритм нахождения максимального значения элементов матрицы.  
2. Разработать параллельную реализацию с использованием **MPI**, распределяющую вычисления между несколькими процессами.  
3. Проверить корректность вычислений и провести экспериментальные замеры времени выполнения.  


## Описание алгоритма
**Последовательный алгоритм** выполняет следующие шаги:

1. Выполняется проверка корректности входных данных, включая проверку размеров матрицы.
2. Производится инициализация переменной результата значением, минимально возможным для используемого типа данных (в данном случае типа double)
3. Осуществляется последовательный проход по всем элементам матрицы, представленной как одномерный массив, с выполнением следующих операций:
   - сравниваем текущее значение с максимумом,
   - обновляет максимум при необходимости.
4. Возвращает максимальное значение.


## Описание схемы параллельного алгоритма
Параллельная версия алгоритма строится на принципе распределения элементов матрицы между несколькими MPI-процессами.   

1. Корневой процесс (`rank 0`) распределяет блоки элементов между процессами через `MPI_Scatter`.  
2. Каждый процесс вычисляет локальные максимум.  
3. Локальные результаты собираются на корневом процессе с помощью `MPI_Gather`.  
4. Корневой процесс находит максимум среди локальных результатов.  
5. Результат рассылается всем процессам через `MPI_Bcast` для обеспечения корректного прохождения функциональных тестов всеми процессами.  

Таким образом, каждый процесс работает с собственной частью данных, что позволяет достичь ускорения при достаточно больших размерах матриц.


## Описание MPI-версии
Программная реализация использует в качестве типа входных данных специализированный кортеж:  
`std::tuple<size_t, std::vector<double>>`, 
где:  
- первый элемент кортежа — количество строк матрицы `n`,  
- второй элемент — вектор значений матрицы, хранящийся в построчном порядке.  

Данная структура данных была выбрана как наиболее удобная для организации эффективного распределения данных между процессами и последующего сбора результатов вычислений.


## Результаты экспериментов и подтверждение корректности
Эксперименты проводились на локальной машине.  
Параметры тестовой матрицы:
- размер 2<sup>11</sup> × 2<sup>12</sup>
- элементы типа `double`.
- использовался генератор псевдослучайных чисел `mt19937`, также известный известный как **«Вихрь Мерсенна»**.
- Диапазон значений элементов матрицы: от -10<sup>6</sup> до 10<sup>6</sup>.


| Версия алгоритма | Время выполнения (с) |
|------------------:|---------------------:|
| Последовательная | 0.051 |
| mpi 1 проц. | 0.077 |
| mpi 2 проц. | 0.045 |
| mpi 4 проц. | 0.036 |
| mpi 6 проц. | 0.034 |
| mpi 8 проц. | 0.031 |
| mpi 16 проц. | 0.078 |

**Подтверждение корректности:**  
Функция тестирования проверяет, что для матрицы найденный максимум не меньше всех элементов.  
Все функциональные тесты и тесты производительности были успешно пройдены на локальной машине.


## Выводы из результатов
Параллельная реализация с использованием MPI демонстрирует ускорение примерно в 1.64 раза при использование 8-ми процессов по сравнению с последовательной версией при запуске на локальной машине.

Это подтверждает эффективность параллельного подхода при обработке крупных матриц. При увеличении числа процессов можно ожидать дальнейшего сокращения времени выполнения для достаточно больших матриц, однако для малых размеров матриц затраты на коммуникацию могут снизить эффективность.


## Заключение
В работе реализованы и протестированы последовательная и параллельная (MPI) версии алгоритма нахождения максимальных значений по столбцам матрицы. Проведённые эксперименты подтвердили корректность и эффективность параллельной реализации.


## Список литературы
1. Документация в формате веб-сайта по реализации **MPICH** стандарта **MPI**: [https://www.mpich.org](https://www.mpich.org)


## Параллельная реализация
```cpp
  int size = 0;
  int rank = 0;
  MPI_Comm_size(MPI_COMM_WORLD, &size);
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);

  std::vector<int> sendcounts(size, 0);
  std::vector<int> displs(size, 0);
  std::vector<double> data;
  std::vector<double> all_maxs(size, std::numeric_limits<double>::lowest());

  OutType &mx = GetOutput();
  std::vector<double> &v = std::get<1>(GetInput());

  MPI_Datatype datatype = MPI_DOUBLE;

  int len = static_cast<int>(v.size());
  MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);

  int cur_disp = 0;
  int cnt_item = len / size;
  int r = len % size;
  for (int i = 0; i < size; ++i) {
    int cur_cnt = cnt_item + (i < r ? 1 : 0);
    sendcounts[i] = cur_cnt;
    displs[i] = cur_disp;
    cur_disp += sendcounts[i];
  }
  data.resize(sendcounts[rank]);

  MPI_Scatterv((rank == 0) ? v.data() : nullptr, sendcounts.data(), displs.data(), datatype, data.data(),
               sendcounts[rank], datatype, 0, MPI_COMM_WORLD);

  bool flag = false;
  OutType tmp_mx = std::numeric_limits<double>::lowest();
  for (int i = 0; i < sendcounts[rank]; ++i) {
    flag = data[i] > tmp_mx;
    tmp_mx = (static_cast<double>(flag) * data[i]) + (static_cast<double>(!flag) * tmp_mx);
  }

  for (int i = 0; i < size; ++i) {
    sendcounts[i] = 1;
    displs[i] = i;
  }

  MPI_Gatherv(&tmp_mx, 1, datatype, all_maxs.data(), sendcounts.data(), displs.data(), datatype, 0, MPI_COMM_WORLD);

  if (rank == 0) {
    for (int i = 0; i < size; ++i) {
      flag = all_maxs[i] > mx;
      mx = (static_cast<double>(flag) * all_maxs[i]) + (static_cast<double>(!flag) * mx);
    }
  }

  MPI_Bcast(&mx, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  MPI_Barrier(MPI_COMM_WORLD);
```