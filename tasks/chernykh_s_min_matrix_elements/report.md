# Минимальное значение элементов матрицы

Студент: Черных Севастьян Владимирович
Группа: 3823Б1ПМоп3
Технология: SEQ, MPI
Вариант: 14

## 1. Введение

Целью данной работы является реализация и исследование эффективности параллельного алгоритма поиска минимального элемента в матрице с использованием технологии MPI. В данной работе применяется метод декомпозиции данных (разбиение матрицы на части) и коллективные операции MPI для эффективного сбора финального результата.

## 2. Постановка задачи

Требуется найти минимальное значение среди всех элементов квадратной матрицы $A$ размерностью $N \times N$, где $A \in \mathbb{R}^{N \times N}$. 
Задача заключается в вычислении:
$$\min_{i, j} \{A_{i, j}\}, \quad \text{где } 1 \le i, j \le N$$


- Входные данные (`InType`): Файл с данными, представляющими квадратную матрицу чисел с плавающей точкой (`double`). 
- ``` c++
    using InType = std::vector<std::vector<double>>;
    ```
- Выходные данные (`OutType`): Одно число с плавающей точкой (`double`), являющееся минимальным элементом матрицы.   ```
``` c++
    using OutType = double;
    ```

## 3. Базовая реализация
Базовый (последовательный) алгоритм для задачи поиска минимального элемента в матрице основан на полном переборе всех элементов.
1. Инициализация: Переменная для хранения минимального элемента, min_element, устанавливается в максимально возможное значение для типа `double`
2. Перебор: Используются два вложенных цикла для последовательного обхода всех строк (i) и всех элементов в этих строках (j) матрицы A.
3. Сравнение и Обновление: На каждой итерации текущий элемент матрицы Ai,j​ сравнивается с min_element. Если Ai,j​ меньше, то min_element обновляется:
    min_element=min(min_element,Ai,j​)
4. Результат: После завершения обхода всех элементов матрицы, переменная min_element будет содержать искомое минимальное значение.

Сложность данного алгоритма составляет O(N2), где N2 — общее количество элементов в матрице размерностью N×N.

## 4. Параллельная реализация
- Технология: Используется MPI (Message Passing Interface).
- Декомпозиция: Одномерная блочная декомпозиция матрицы по строкам.
- Распределение нагрузки: Строки делятся на P процессов; остаток строк назначается последнему процессу (`rank == size - 1`).
- Локальные вычисления: Каждый процесс ищет локальный минимум (`local_min`) в своем блоке строк.
- Коммуникация: Для получения финального результата используется коллективная операция `MPI_Allreduce`.
- Операция редукции: Выполняется операция `MPI_MIN` для нахождения глобального минимума среди всех `local_min`.
- Результат: Глобальный минимум (`global_min`) становится доступен всем процессам.

## 5. Детали реализации

### 5.1 Общий интерфейс (common/include)

В директории `common/include` находится заголовочный файл `common.hpp`, содержащий определение типов входных (`InType`) и выходных (`OutType`) данных, а также тип входных данных тестовых кейсов (`TestType`; в данном случае строка `string`, представляющая имя файла с тестовыми данными) и базовый класс интерфейса (`BaseTask`).
### 5.2 Формат тестовых данных (data)

Директория `data` содержит текстовые файлы `.txt` с маской имени `create_data_<NxN>`. В файлах записаны данные матрицы размеров NxN для _функциональных_ тестов. Файл `create_data_2048x2048.txt` содержит тестовую информацию для _performance_-тестов.
Координаты векторов во всех файлах сгенерированы в отдельном приложении с использованием генератора псевдо-случайных чисел типа _Mersenne Twister_ `std::mt19937` по равномерному распределению `std::uniform_real_distribution` в отрезке [-50000.0, 50000.0]. Так же в случайное место внедряется минимальное значение -9000000.
### 5.3 Интерфейс реализаций алгоритмов (mpi & seq)

В директориях `mpi` и `seq` расположены `.hpp` и `.cpp` файлы соответствующих реализаций. Общий интерфейс включает методы валидации данных `bool ValidationImpl(void)`, предобработки данных `bool PreProcessingImpl(void)` (в нашей задаче она не требуется, выдает true), вычисления минимального элемента матрицы `bool RunImpl(void)` (алгоритмы MPI и SEQ реализаций описаны в соответствующих пунктах отчета) и постобработки данных `bool PostProcessingImpl(void)` (в наше задаче не требуется).
### 5.4 Тестирование (tests)

Проверка корректности и анализ производительности реализованы в директории `tests`, которая содержит как функциональные, так и _performance_-тесты. Оба типа тестов используют одинаковую структуру для исполнения. Перед выполнением алгоритма, в функции `void SetUp(void)`, происходит инициализация путем чтения необходимых входных данных для векторов из файла. После завершения вычислительной части, корректность полученного результата проверяется методом `bool CheckTestOutputData(OutType& output_data)`. Этот метод сравнивает выходные данные программы с посчитанным минимумом.
### 5.5 Управление памятью
Управление памятью осуществляется автоматически средствами контейнера `std::vector`. Динамические указатели с ручным управлением памятью не используются.

## 6. Тестовая конфигурация

- Процессор: AMD Ryzen 5 5600X 6-Core Processor
- Память: 32 GB DDR4
- Операционная система: Windows 10 
- Компилятор: GCC 15.2.0, ключи: -O2
- Реализация MPI: MS-MPI version 10.1.12498.52
- Параметры запуска: `mpiexec -n <count>`
- Данные: матрица размерность 2000x2000, сгенерированная с соответствии с пунктом 2



## 7. Экспериментальные результаты

| Процессы P | Время TP​ (сек) | Ускорение ​​ | Эффективность ​​ |
| ---------- | --------------- | ------------ | ---------------- |
| 1 (SEQ)    | 0.0025353982    | 1.00         | 100.0%           |
| 2          | 0.0013880936    | 1.83         | 91.3%            |
| 4          | 0.0026315432    | 0.96         | 24.1%            |
| 6          | 0.0080683120    | 0.31         | 5.2%             |
- Наилучший результат достигнут при использовании P=2 процессов (S2​≈1.83), что подтверждает высокую эффективность параллелизма при низком уровне коммуникаций.
- Наблюдается замедление при P≥4. Ускорение падает ниже 1, что означает, что запуск программы на 4 и 6 процессах медленнее, чем последовательная реализация.
- Результаты эксперимента подтверждают неэффективность использования MPI для задач с низким отношением вычислений к объему коммуникаций на машинах с общей памятью. Накладные расходы на инициализацию и глобальную коммуникацию (MPI) в данном случае превосходят вычислительную нагрузку, что приводит к кратному замедлению программы при использовании P=6 процессов.

## 8. Заключение
Использовать MPI для такой **простой и быстрой** задачи на вашем компьютере (с общей памятью) **невыгодно**. Накладные расходы на MPIсъедают весь выигрыш.

