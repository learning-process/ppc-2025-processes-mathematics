# Минимальное значение элементов матрицы

Студент: Черных Севастьян Владимирович
Группа: 3823Б1ПМоп3
Технология: SEQ, MPI
Вариант: 14

## 1. Введение

Целью данной работы является реализация и исследование эффективности параллельного алгоритма поиска минимального элемента в матрице с использованием технологии MPI. В данной работе применяется метод декомпозиции данных (разбиение матрицы на части) и коллективные операции MPI для эффективного сбора финального результата.

## 2. Постановка задачи

Требуется найти минимальное значение среди всех элементов квадратной матрицы $A$ размерностью $N \times N$, где $A \in \mathbb{R}^{N \times N}$.
Задача заключается в вычислении:
$$\min_{i, j} \{A_{i, j}\}, \quad \text{где } 1 \le i, j \le N$$


- Входные данные (`InType`): Файл с данными, представляющими квадратную матрицу чисел с плавающей точкой (`double`).
- ``` c++
    using InType = std::vector<std::vector<double>>;
    ```
- Выходные данные (`OutType`): Одно число с плавающей точкой (`double`), являющееся минимальным элементом матрицы.   ```
``` c++
    using OutType = double;
    ```

## 3. Базовая реализация
Базовый (последовательный) алгоритм для задачи поиска минимального элемента в матрице основан на полном переборе всех элементов.
1. Инициализация: Переменная для хранения минимального элемента, min_element, устанавливается в максимально возможное значение для типа `double`
2. Перебор: Используются два вложенных цикла для последовательного обхода всех строк (i) и всех элементов в этих строках (j) входной матрицы.
3. Сравнение и Обновление: На каждой итерации текущий элемент матрицы A[i,j​] сравнивается с min_element. Если A[i,j​] меньше, то min_element обновляется:
    min_element=min(min_element,A[i,j​])
4. Результат: После завершения обхода всех элементов матрицы, переменная min_element будет содержать искомое минимальное значение.

Сложность данного алгоритма составляет O(N^2), где N^2 — общее количество элементов в матрице размерностью N×N.

## 4. Параллельная реализация
- Технология: Используется MPI (Message Passing Interface).
- Декомпозиция: Одномерная блочная декомпозиция матрицы по строкам.
- Распределение нагрузки: Строки делятся на общее количество процессов; остаток строк назначается последнему процессу (`rank == size - 1`).
- Локальные вычисления: Каждый процесс ищет локальный минимум в своем блоке.
- Коммуникация: Для получения финального результата используется коллективная операция `MPI_Allreduce`.
- Операция редукции: Выполняется операция `MPI_MIN` для нахождения глобального минимума среди всех `local_min`.
- Результат: Глобальный минимум (`global_min`).

## 5. Детали реализации

### 5.1 Общий интерфейс (common/include)

В директории `common/include` находится заголовочный файл `common.hpp`, содержащий определение типов входных (`InType`) и выходных (`OutType`) данных, а также тип входных данных тестовых кейсов (`TestType`; в данном случае строка `string`, представляющая имя файла с тестовыми данными) и базовый класс интерфейса (`BaseTask`).
### 5.2 Формат тестовых данных (data)

Директория `data` содержит текстовые файлы `.txt` с маской имени `create_data_<NxN>`. В файлах записаны данные матрицы размеров NxN для _функциональных_ тестов. Координаты векторов во всех файлах сгенерированы в отдельном приложении с использованием генератора псевдо-случайных чисел типа _Mersenne Twister_ `std::mt19937` по равномерному распределению `std::uniform_real_distribution` в отрезке [-500.0, 500.0]. Так же в случайное место внедряется минимальное значение -1000.0.
### 5.3 Интерфейс реализаций алгоритмов (mpi & seq)

В директориях `mpi` и `seq` расположены `.hpp` и `.cpp` файлы соответствующих реализаций. Общий интерфейс включает методы валидации данных `bool ValidationImpl(void)`, предобработки данных `bool PreProcessingImpl(void)` (в нашей задаче она не требуется, выдает true), вычисления минимального элемента матрицы `bool RunImpl(void)` (алгоритмы MPI и SEQ реализаций описаны в соответствующих пунктах отчета) и постобработки данных `bool PostProcessingImpl(void)` (в нашей задаче не требуется).
### 5.4 Тестирование (tests)

Проверка корректности и анализ производительности реализованы в директории `tests`, которая содержит как функциональные, так и _performance_-тесты. Оба типа тестов используют одинаковую структуру для исполнения. Перед выполнением алгоритма, в функции `void SetUp(void)`, происходит инициализация путем чтения необходимых входных данных для векторов из файла, или создание матрицы с помощью функции `GenerateMatrix(n)` для _perfomance_ тестов. После завершения вычислительной части, корректность полученного результата проверяется методом `bool CheckTestOutputData(OutType& output_data)`. Этот метод сравнивает выходные данные программы с посчитанным минимумом.
### 5.5 Управление памятью
Управление памятью осуществляется автоматически средствами контейнера `std::vector`. Динамические указатели с ручным управлением памятью не используются.

## 6. Тестовая конфигурация

- Процессор: AMD Ryzen 5 5600X 6-Core Processor
- Память: 32 GB DDR4
- Операционная система: Windows 10 
- Компилятор: GCC 13.3.0
- Реализация MPI: MS-MPI version 10.1.12498.52
- Параметры запуска: `mpiexec -n <count>`
- Данные: матрица размерность 8192x8192, сгенерированная с соответствии с пунктом 5.2

## 7. Экспериментальные результаты

| Процессы P | Время TP​ (сек) | Ускорение ​​ | Эффективность ​​ |
| ---------- | --------------- | ------------ | ---------------- |
| 1 (SEQ)    | 0.0292124748    | 1.00         | 100.0%           |
| 2          | 0.8653819954    | 0.03375      | 1.69%            |
| 4          | 0.7607035520    | 0.03840      | 0.96%            |
| 6          | 0.7448199514    | 0.03922      | 0.65%            |
- Фактическое ускорение во всех параллельных случаях значительно меньше 1. Это указывает на то, что параллельная версия работает медленнее, чем последовательная.
- Эффективность падает до уровня менее 1% при 4 и 6 процессах, подтверждая, что накладные расходы на MPI (коммуникация и инициализация) в данной задаче превосходят полезную вычислительную работу.

## 8. Заключение
В рамках учебного проекта были разработаны и протестированы две версии программы для поиска минимального элемента в матрице: последовательная (SEQ) и параллельная (MPI). Проведённый эксперимент чётко показал, что использование MPI нецелесообразно для такой задачи на компьютере с общей памятью. Эксперимент подтверждает, что для нетрудоемких алгоритмов, где время на общение между процессами (коммуникация) больше, чем время на сами вычисления, использование MPI на машинах с общей памятью приводит к замедлению программы, а не к ускорению.
