# Интегрирование – метод Монте-Карло

- Студент: <Краснопевцева Вероника Дмитриевна>, группа 3823Б1ПМоп3
- Технологии: SEQ | MPI 
- Вариант: 21

## 1. Введение
Численное интегрирование является фундаментальной задачей вычислительной математики. Для сложных функций, не имеющих аналитического решения, применяются численные методы. Метод Монте-Карло - мощный инструмент для численного интегрирования сложных функций, где аналитические методы затруднены. Он используется как для обычных определенных интегралов, так и для многомерных интегралов и ряда других математических задач.
Цель работы - реализация параллельной версии метода Монте-Карло для вычисления интеграла функций с использованием MPI и сравнение эффективности с последовательной версией. 

## 2. Постановка задачи
### Задача: реализация последовательной (SEQ) и параллельной (MPI) версий метода Монте-Карло интегрирования нелинейной функции.
Разработана универсальная система для численного интегрирования четырех различных функций:
1. f1(x) = cos(x)*x^3 - функция с возрастающей амплитудой
2. f2(x) = sin(x)*x^2 - функция с квадратичным ростом  
3. f3(x) = exp(-x)*x - затухающая экспоненциальная функция
4. f4(x) = x^4 - 2x^2 + 1 - полиномиальная функция

Для каждой функции реализовано аналитическое решение, что позволяет:
- Проверять корректность численного метода
- Сравнить поведение метода на функциях различного поведения

### Формат входных данных:
- Нижняя граница интегрирования a типа double
- Верхняя граница интегрирования b типа double
- Количество случайных точек n типа int, n > 0
- Индекс функции типа std::uint8_t

### Формат выходных данных:
- Приближенное значение интеграла I типа double

### Ограничения:
- Интервал интегрирования должен быть корректным (a < b)
- Количество точек должно быть положительным
- Результат должен быть конечным числом
- Индекс функции должен быть от 0 до 3

## 3. Описание алгоритма
### Базовый алгоритм Монте-Карло:

1. Задание границ интегрирования [a, b] и определение количества точек n

2. Создание n равномерно распределенных по отрезку точек  
   *В качестве генератора случайных чисел использовался псевдослучайный генератор вихрь Мерсенна(std::mt19937 gen(rd())) с функцией равномерного распределения точек от a до b c помощью std::uniform_real_distribution<double> dis(a, b).

3. Для каждой точки в цикле вычисляется значение подинтегральной функции и сумма этих значений накапливается в переменной sum

4. Оценка интеграла по формуле I = (b-a)*sum/n

При достаточно большом n оценка стремится к истинному значению интеграла. Погрешность метода имеет порядок O(1/sqrt(n)).

## 4. Схема распараллеливания
### Основная нагрузка
Основной нагрузкой на метод является именно большое количество случайных точек на интервале для хорошей точности, поэтому было реализовано распределение данных как распределение количества точек между процессами.
### Распределение количества точек
- количество точек процессу = общее кол-во точек / число потоков.
- остаток = остаток от деления общего кол-ва точек на число потоков.
- если ранг процесса < остатка => то количество точек процессу увеличивается на 1.
### Равномерное распределение точек
Начальное значение для генератора псевдослучайных чисел разное для каждого процесса (std::mt19937 gen(std::random_device{}() + rank)), чтобы обеспечить разные последовательности между процессами в одном запуске.

| Этап | Процесс 0 | Процесс 1 | Процесс 2 | Процесс 3 |
|------|-----------|-----------|-----------|-----------|
| 1 | Генерация+Вычисление | Генерация+Вычисление | Генерация+Вычисление | Генерация+Вычисление |
| 2 | MPI_Reduce (root) | MPI_Reduce | MPI_Reduce | MPI_Reduce |
| 3 | Вычисление integral | - | - | - |
| 4 | MPI_Bcast | MPI_Bcast | MPI_Bcast | MPI_Bcast |

## 5. Детали реализации 

Ключевые классы и функции: 
- класс KrasnopevtsevaVMCIntegrationMPI с реализацией параллельной версии метода
- класс KrasnopevtsevaVMCIntegrationSEQ с реализацией последовательной версии метода
- определение пространства имен namespace krasnopevtseva_v_monte_carlo_integration с определеним используемых типов данных:  
    using InType = std::tuple<double, double, int, std::uint8_t>; - тип входных данных  
    using OutType = double; - тип возвращаемых данных(значение интеграла)  
    using TestType = std::tuple<std::tuple<double, double, int, std::uint8_t>, std::string>;
    using BaseTask = ppc::task::Task<InType, OutType>;   

## 6. Окружение
- Windows: AMD Ryzen 7 5700X 8-Core Processor, 32.0 ГБ ОП, Windows 11 Pro 25H2
- Набор инструментов: DevContainer:compiler gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0, Open MPI 4.1.6, build type Release

## 7. Результаты

### 7.1 Корректность
Были найдены аналитические решения интегралов для сравнения корректности результата.   
Погрешность считалась по формуле tolerance = (b - a) / sqrt(points) * 10   
Точность решения ухудшается при выходе из интервала интегрирования [-3,3] для функций sin(x)*x^2 и cos(x)*x^3 потому что после этих значений график функции сильно растягивается - при выходе за эти границы для функций с индексами 0 и 1 увеличиваем допустимую погрешность.  

### 7.2 Производительность


| Mode        | Count | Time,s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 1.316   | 1.00    | N/A        |
| mpi         | 2     | 0.666   | 1.98    | 99.0%      |
| mpi         | 4     | 0.344   | 3.82    | 95.5%      |

Расчеты:  
- Speedup = T_seq / T_parallel  
- Для 2 процессов: 1.316 / 0.666 = 1.98
- Для 4 процессов: 1.316 / 0.344 = 3.82

- Efficiency = Speedup / Count × 100%
- Для 2 процессов: 1.98 / 2 × 100% = 99.0%
- Для 4 процессов: 3.82 / 4 × 100% = 95.5%

## 8. Заключение

Вывод: Реализация интегрирования методом Монте-Карло демонстрирует отличную масштабируемость и может эффективно использоваться на многопроцессорных системах.

## 9. Приложения
```cpp
bool KrasnopevtsevaVMCIntegrationMPI::RunImpl() {
  const auto &input = GetInput();
  double a = std::get<0>(input);
  double b = std::get<1>(input);
  int num_points = std::get<2>(input);
  int func = std::get<3>(input);

  int rank = 0;
  int size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);
  int local_points = num_points / size;
  int remainder = num_points % size;

  if (rank < remainder) {
    local_points++;
  }
  double local_sum = 0.0;
  std::mt19937 gen(std::random_device{}() + rank);
  std::uniform_real_distribution<double> dis(a, b);

  for (int i = 0; i < local_points; i++) {
    double x = dis(gen);
    double fx = FuncSystem::getFunc(func, x);
    local_sum += fx;
  }
  double global_sum = 0.0;
  MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
  double integral = 0.0;
  if (rank == 0) {
    integral = (b - a) * global_sum / num_points;
  }
  MPI_Bcast(&integral, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  GetOutput() = integral;
  return true;
}
```