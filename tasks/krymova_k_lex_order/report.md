# Сравнение лексикографического порядка строк с использованием SEQ и MPI технологий

- **Студент**: Крымова Кристина Дмитриевна, 3823Б1ПМоп3 
- **Технология**: SEQ, MPI
- **Вариант**: 26

## 1. Введение
- **Мотивация**: Изучить применение MPI для параллельного сравнения строк лексикографическим порядком, оценить эффективность распределения вычислений между процессами.
- **Проблема**: Задача сравнения строк лексикографическим порядком возникает во многих приложениях (сортировка, поиск, анализ текста). При работе с очень длинными строками последовательный алгоритм может быть медленным.
- **Ожидаемый результат**: MPI версия должна показать ускорение за счет распределения сравнения символов между несколькими процессами.

## 2. Постановка задачи
На вход программе подаются 2 строки. Требуется определить их лексикографический порядок:
- Возвращает -1, если первая строка меньше второй
- Возвращает 0, если строки равны  
- Возвращает 1, если первая строка больше второй

Алгоритм сравнения:
1. Сравнивать символы строк попарно до первого различия
2. Если найдено различие - определить порядок на основе этих символов
3. Если различий нет - сравнить длины строк

## 3. Базовый алгоритм (последовательный)
Последовательный алгоритм проходит по символам обеих строк до первого различия или до конца более короткой строки.

```cpp
bool KrymovaKLexSEQ::RunImpl() {
  const std::string &str1 = std::get<0>(GetInput());
  const std::string &str2 = std::get<1>(GetInput());

  size_t len1 = str1.length();
  size_t len2 = str2.length();
  size_t min_len = (len1 < len2) ? len1 : len2;
  
  for (size_t i = 0; i < min_len; ++i) {
    if (str1[i] != str2[i]) {
      GetOutput() = (str1[i] < str2[i]) ? -1 : 1;
      return true;
    }
  }
  
  if (len1 < len2) {
    GetOutput() = -1;
  } else if (len1 > len2) {
    GetOutput() = 1;
      } else {
    GetOutput() = 0;
  }
  return true;
}
```
## 4. Описание параллельного алгоритма

Параллельный алгоритм основан на последовательной версии и предназначен для распределения сравнения символов между несколькими процессами. Алгоритм работает следующим образом:

1. **Распределение данных**:
   - Определяется минимальная длина из двух строк (`min_len`)
   - Диапазон сравнения разбивается на равные части (`chunk_size`)
   - Каждый процесс получает свой непрерывный участок строк через `MPI_Scatter`

2. **Локальное сравнение**:
   - Каждый процесс последовательно проверяет свой участок
   - При обнаружении первого различия запоминается его позиция
   - Если различий в локальном диапазоне нет, позиция устанавливается в `min_len`

3. **Глобальная синхронизация**:
   - Используется `MPI_Allreduce` с операцией `MPI_MIN` для нахождения минимальной позиции различия среди всех процессов
   - Это позволяет определить самое первое различие в строках

4. **Определение результата**:
   - Если глобальная позиция различия меньше `min_len`, сравниваются символы в этой позиции
   - Символы передаются через `MPI_Bcast` от корневого процесса
   - Если различий нет, результат определяется на основе длин строк

Алгоритм минимизирует коммуникационные затраты, используя всего одну коллективную операцию (`MPI_Allreduce`) для определения позиции первого различия и две операции `MPI_Bcast` для передачи символов сравнения.

## 5. Experimental Setup

**Hardware/OS**: Apple MacBook Air with Apple M1 Chip (8 cores: 4 performance + 4 efficiency), 8 GB RAM, macOS

**Toolchain**: Apple Clang version 16.0.0, сборка через VSCode с оптимизацией (-O2/-O3)

**Data**: Для тестирования производительности использовались строки длиной 10,000,000 символов с различием в последнем символе

## 6. Результаты

### 6.1 Корректность

Корректность алгоритмов проверена 8 функциональными тестами, покрывающими различные сценарии:

- Равные строки
- Разные первые символы
- Различия в середине строк
- Строки разной длины

### 6.2 Производительность

Для оценки производительности использовались строки длиной **100 миллионов символов**. Базовое время выполнения последовательной (SEQ) версии: **0.06272 секунд**.

#### Результаты параллельного выполнения:

| Processes | MPI Time (s) | Speedup vs SEQ |
|-----------|--------------|----------------|
| 1         | 0.08214      | 0.78x          | 
| 2         | 0.05103      | 1.25x          |
| 4         | 0.04392      | 1.45x          | 
| 6         | 0.10018      | 0.64x          | 
| 8         | 0.18784      | 0.34x          | 

#### Анализ результатов:

**Положительные аспекты:**
1. **Максимальное ускорение**: 1.45x достигается при 4 процессах
2. **Масштабируемость на малом числе процессов**: Алгоритм демонстрирует положительное ускорение при 2-4 процессах

**Проблемные зоны:**
1. **Обратная масштабируемость**: При 6 и 8 процессах наблюдается замедление 

**Причины снижения производительности:**
- **Накладные расходы MPI**: Время на коммуникацию начинает преобладать над временем вычислений
- **Конкуренция за ресурсы**: На системе с 8 ГБ ОЗУ и 8 ядрами использование всех ресурсов приводит к конфликтам

## 7. Выводы

1. **Эффективность MPI**: MPI версия демонстрирует ускорение до 1.43x при оптимальном количестве процессов (4)
2. **Ограниченная масштабируемость**: Алгоритм эффективно масштабируется только до 4 процессов для данного размера задачи
3. **Важность выбора количества процессов**: Критически важно подбирать количество процессов в зависимости от размера входных данных
4. **Практическая применимость**: Для задач лексикографического сравнения больших строк (сотни миллионов символов) использование 2-4 MPI процессов дает ускорение

## 8. Литература
1. Стандарт MPI.
2. Лекции и практики по параллельному программированию.

## 9. Приложение

```cpp
bool KrymovaKLexOrderMPI::RunImpl() {
  const std::string &str1 = std::get<0>(GetInput());
  const std::string &str2 = std::get<1>(GetInput());

  int rank = 0;
  int size = 1;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  int len1 = 0, len2 = 0;
  if (rank == 0) {
    len1 = static_cast<int>(str1.length());
    len2 = static_cast<int>(str2.length());
  }
  
  MPI_Bcast(&len1, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&len2, 1, MPI_INT, 0, MPI_COMM_WORLD);

  int min_len = std::min(len1, len2);
  int chunk_size = (min_len + size - 1) / size;
  int total_size = chunk_size * size;

  std::vector<char> sendbuf1(total_size, 0);
  std::vector<char> sendbuf2(total_size, 0);
  std::vector<char> local_str1(chunk_size, 0);
  std::vector<char> local_str2(chunk_size, 0);

  if (rank == 0) {
    std::copy(str1.begin(), str1.begin() + min_len, sendbuf1.begin());
    std::copy(str2.begin(), str2.begin() + min_len, sendbuf2.begin());
  }

  MPI_Scatter(sendbuf1.data(), chunk_size, MPI_CHAR, 
              local_str1.data(), chunk_size, MPI_CHAR, 0, MPI_COMM_WORLD);
  MPI_Scatter(sendbuf2.data(), chunk_size, MPI_CHAR, 
              local_str2.data(), chunk_size, MPI_CHAR, 0, MPI_COMM_WORLD);

  int start = rank * chunk_size;
  int end = std::min(start + chunk_size, min_len);
  int actual_size = end - start;

  int local_diff_pos = min_len; 
  
  for (int i = 0; i < actual_size; ++i) {
    if (local_str1[i] != local_str2[i]) {
      local_diff_pos = start + i;
      break;
    }
  }

  int global_first_diff = min_len;
  MPI_Allreduce(&local_diff_pos, &global_first_diff, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);

  int result = 0;
  
  if (global_first_diff < min_len) {
    char char1 = 0, char2 = 0;
    
    if (rank == 0) {
      char1 = str1[global_first_diff];
      char2 = str2[global_first_diff];
    }
    
    MPI_Bcast(&char1, 1, MPI_CHAR, 0, MPI_COMM_WORLD);
    MPI_Bcast(&char2, 1, MPI_CHAR, 0, MPI_COMM_WORLD);
    
    result = (char1 < char2) ? -1 : 1;
  } else {
    if (len1 < len2) {
      result = -1;
    } else if (len1 > len2) {
      result = 1;
    } else {
      result = 0;
    }
  }

  GetOutput() = result;
  return true;
}