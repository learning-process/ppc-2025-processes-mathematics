# Нахождение наиболее отличающихся по значению соседних элементов вектора

- Студент: Кулик Артур Игоревич, группа 3823Б1ПМоп3
- Технология: SEQ, MPI
- Вариант: 8

## 1. Введение
Для решения сложных вычислительных задач (hpc) на практике используются кластерные системы с большим количеством многоядерных процессоров. Для коммуникации между процессами, запущенными на кластере, используют технологию MPI (Message Passing Interface). Цель работы - распараллелить алгоритм поиска максимальной разности между соседними элементами с использованием MPI для уменьшения времени выполнения при работе с большим вектором на нескольких потоках. 

## 2. Постановка задачи
Задача: Найти наиболее отличающиеся по значению соседние элементы вещественного вектора.

**Входные данные:** вектор `std::vector<double>`.
**Выходные данные:** пара целых чисел - позиции элементов `std::pair<int, int>`.

**Ограничения к входным данным:** длина вектора не меньше 2.

**Необходимо реализовать:**
1. последовательную (**SEQ**) и параллельную (**MPI**) версию алгоритма.
2. Покрытие кода тестами, проверяющими функционал и производительность. 

## 3. Описание последовательного алгоритма
Формализируя задачу, необходимо найти следующую величину: 
max(|A[ i ] - A[ i - 1 ]|) для i = 1, ..., (n-1); где n - это размер вектора.
Алгоритм представляет собой одноразовый проход по вектору, где на каждой итерации мы сравниваем модуль разницы двух соседних элементов с текущим максимумом разности. Если эта разница превышает максимум, то сама становится максимумом, а индекс элементов, на которых она достигнута, - новым ответом. 

## 4. Схема распараллеливания
Для реализации MPI-программы используется топология стандартного коммуникатора `MPI_COMM_WORLD`. Нулевой процесс является корневым. В реализации используется стандартная идея при работе с MPI - увеличение размерности входных данных для равномерного распределения между процессами.

Разобьём алгоритм на несколько этапов:
1. Находим остаток от деления размера вектора на число процессов. Так как нельзя менять входные данные и желательно избежать копирования вектора для хорошей производительности, то сначала находим `update_n` - обновленную длину вектора - как если бы в него вписали недостающие элементы. После этого находим размеры векторов, которые передадим каждому процессу: `size = update_n / ProcNum`, `std::vector<int> elemcnt(ProcNum, size)`, `elemcnt[ProcNum - 1] = n - size * (ProcNum - 1)`. Заводим векторы буферов: `std::vector<double> buf(elemcnt[ProcRank])`. Далее с помощью `MPI_Scatterv` распределяем данные между процессами. Вектор `buf` на последнем потоке дополняем до размера `size` последним элементом входного вектора. Таким образом, мы получили векторы одинакового размера `size` на всех процессах.
2. Заводим структуру {
    `double val;`
    `int ind;`
  } для упрощения реализации. Далее основная вычислительная часть - на каждом процессе фактически запускается последовательный алгоритм. 
3. Необходимо обработать граничные случаи: может получиться так, что максимальная разность будет достигаться между последним элементом одного вектора и первым элементом вектора на следующим процессе. Для этого с помощью функции `MPI_Send` с каждого процесса за исключением последнего высылаем последующему последний элемент своего вектора `buf`. Аналогично воспользуемся функцией `MPI_Recv` для принятия сообщений от предыдущего процесса на каждом процессе, кроме нулевого. Сравниваем разность переданного последнего элемента и первого элемента вектора на процессе с максимальной разностью, которая была достигнута на данном процессе. Если разница последнего и первого элемента оказалась больше, то теперь это новый максимум на данном процессе.
4. Аккумулируем результат с помощью функции `MPI_Allreduce`. Для передачи нашей структуры используем `MPI_Datatype = MPI_DOUBLE_INT` и операцию `MPI_Op = MPI_MAXLOC`, которая возвращает пару с наибольшим значением val и соответствующим ему индексом.

## 5. Детали реализации
1. Основная информация о структуре решения (ввод/вывод) находится в файле `common.hpp`. `InType` - тип входных данных, в данном случае - `std::vector<double>`, OutType - тип выходных данных, в нашем случае - `std::pair<int, int>`. `TestType` - тип входных данных для тестов, в данном случае - `std::string`, т.к в тестах считываются названия бинарных файлов. `BaseTask` - это базовый класс интерфейса.
2. В папке `data` находятся бинарные файлы: `vector1.bin` и `vector2.bin`. `vector1` имеет размер `100'000` и содержит числа формата `double`, `vector2` имеет размер `9'000'005` и содержит числа формата `double`. Векторы были сгенерированы на локальной машине с помощью генератора псевдослучайных чисел `MT19937`, в качестве `seed` использовался `std::random_device`. Подставляем результат `MT19937` в `std::uniform_real_distribution<double> dist(-1000.0, 1000.0)` - равномерное вещественное распределение на отрезке `[-1000; 1000]`. Файлы содержат также размеры векторов. 

## 6. Конфигурация системы запуска
- локальная машина: intel core i5 9600k, 6 cores / 6 threads, 16GB DDR4, Windows 10
- Окружение и ключи: MSVC, 1931, Release, -O2, MS-MPI, mpiexec -n 4
- Данные: вектор длиной `9'000'005`, сгенерированный как описано в пункте 5.

## 7. Экспериментальные результаты

### 7.1 Корректность 
Корректность работы алгоритма проверена в тестах. 

### 7.2 Производительность

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.133   | 1.00    | N/A        |
| mpi         | 2     | 0.101   | 1.32    | 66.0%      |
| mpi         | 4     | 0.065   | 2.05    | 51.25%      |

Мы можем видеть хороший прирост производительности при увеличении количества процессов. Это говорит нам о том, что алгоритм хорошо параллелится и временные затраты на обслуживание MPI окупаются параллельной работой программы.

## 8. Заключение
Были реализованы последовательная (**SEQ**) и параллельная (**MPI**) версия алгоритма. Также были проведены сравнения производительности данных реализаций, исходя из результатов, можем судить об успешном распараллеливании программы. Запуск на 4-х процессах ещё увеличил ускорение, получаемое на 2-х процессах. Из этого можем высказать предположение, что ресурс параллелизма ещё не исчерпан, вполне возможно получить большее ускорение на большем числе процессов. 

## 9. Источники
https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi