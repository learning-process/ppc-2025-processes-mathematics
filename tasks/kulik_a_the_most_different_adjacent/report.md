# Нахождение наиболее отличающихся по значению соседних элементов вектора

- Студент: Кулик Артур Игоревич, группа 3823Б1ПМоп3
- Технология: SEQ, MPI
- Вариант: 8

## 1. Введение
Для решения сложных вычислительных задач (hpc) на практике используются кластерные системы с большим количеством многоядерных процессоров. Для коммуникации между процессами, запущенными на кластере, используют технологию MPI (Message Passing Interface). Цель работы - распараллелить алгоритм поиска максимальной разности между соседними элементами с использованием MPI для уменьшения времени выполнения при работе с большим вектором на нескольких потоках. 

## 2. Постановка задачи
Задача: Найти наиболее отличающиеся по значению соседние элементы вещественного вектора.

**Входные данные:** вектор `std::vector<double>`.
**Выходные данные:** пара целых неотрицательных чисел - позиции элементов `std::pair<uint64_t, uint64_t>`.

**Ограничения к входным данным:** длина вектора не меньше 2.

**Необходимо реализовать:**
1. последовательную (**SEQ**) и параллельную (**MPI**) версию алгоритма.
2. Покрытие кода тестами, проверяющими функционал и производительность. 

## 3. Описание последовательного алгоритма
Формализируя задачу, необходимо найти следующую величину: 
max(|A[ i ] - A[ i - 1 ]|) для i = 1, ..., (n-1); где n - это размер вектора.
Алгоритм представляет собой одноразовый проход по вектору, где на каждой итерации мы сравниваем модуль разницы двух соседних элементов с текущим максимумом разности. Если эта разница превышает максимум, то сама становится максимумом, а индекс элементов, на которых она достигнута, - новым ответом. 

## 4. Схема распараллеливания
Для реализации MPI-программы используется топология стандартного коммуникатора `MPI_COMM_WORLD`. Нулевой процесс является корневым.

Разобьём алгоритм на несколько этапов:
1. Определяется количество активных процессов: `active_procs = min(n, proc_num)`.Для процессов с рангом `≥ active_procs` выделяется `0` элементов. Элементы равномерно распределяются с учетом остатка: Первые `r` процессов получают `size + 1` элементов, остальные активные процессы получают `size` элементов.
2. Функция распределения данных `CalculateDistribution` рассчитывает массивы `elemcnt` - количество элементов на процесс и `startpos` - начальные позиции.Распределение выполняется только на нулевом (корневом) процессе с последующей рассылкой с помощью `MPI_Bcast`.
3. Функция поиска локального максимума `FindLocalMax` выполняет последовательный проход по локальной части вектора, использует глобальные индексы через параметр `start_index` и находит максимальную разность между соседними элементами в пределах локального блока.
4. Функция обработки граничных случаев `CheckBoundaries` каждый процесс (кроме последнего) отправляет свой последний элемент следующему процессу, каждый процесс (кроме нулевого) принимает последний элемент от предыдущего процесса. Порядок отправки и принятия изменён (сначала `MPI_Recv`, затем `MPI_Send`) с целью избежать дедлока. Проверяется разность между граничными элементами соседних процессов, при необходимости обновляется локальный максимум и индекс.
4. Аккумулирование результата результата происходит в два этапа с помощью функции `MPI_Allreduce`: 
1) Нахождение глобального максимума - мы определяем максимальное значение разности среди всех процессов.
2) Определение индекса глобального максимума - сравниваем локальный максимум на каждом процессе и глобальный максимум, если значения совпадают (с погрешностью на floating point), то определяется позиция достижения максимальной разности. 
## 5. Детали реализации
1. Основная информация о структуре решения (ввод/вывод) находится в файле `common.hpp`. `InType` - тип входных данных, в данном случае - `std::vector<double>`, OutType - тип выходных данных, в нашем случае - `std::pair<uint64_t, uint64_t>`. `TestType` - тип входных данных для тестов, в данном случае - `std::string`, т.к в тестах считываются названия бинарных файлов. `BaseTask` - это базовый класс интерфейса.
2. В папке `data` находятся бинарные файлы: `vector1.bin`, `vector2.bin` и `vector3.bin`. `vector1` имеет размер `100'000` и содержит числа формата `double`, `vector2` имеет размер `9'000'005` и содержит числа формата `double`, `vector3` имеет размер `3` и содержит числа формата `double` - нужен для проверки случая, когда `n` < `proc_num`. Векторы были сгенерированы на локальной машине с помощью генератора псевдослучайных чисел `MT19937`, в качестве `seed` использовался `std::random_device`. Подставляем результат `MT19937` в `std::uniform_real_distribution<double> dist(-1000.0, 1000.0)` - равномерное вещественное распределение на отрезке `[-1000; 1000]`. Файлы содержат также размеры векторов. 
3. Были добавлены вспомогательные функции `CalculateDistribution`, `FindLocalMax`, `CheckBoundaries` для упрощения логики основной функции `RunImpl`.

## 6. Конфигурация системы запуска
- локальная машина: intel core i5 9600k, 6 cores / 6 threads, 16GB DDR4, Windows 10
- Окружение и ключи: MSVC, 1931, Release, -O2, MS-MPI, mpiexec -n 4
- Данные: вектор длиной `9'000'005`, сгенерированный как описано в пункте 5.

## 7. Экспериментальные результаты

### 7.1 Корректность 
Корректность работы алгоритма проверена в тестах. 

### 7.2 Производительность

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.133   | 1.00    | N/A        |
| mpi         | 2     | 0.101   | 1.32    | 66.0%      |
| mpi         | 4     | 0.065   | 2.05    | 51.25%      |

Мы можем видеть хороший прирост производительности при увеличении количества процессов. Это говорит нам о том, что алгоритм хорошо параллелится и временные затраты на обслуживание MPI окупаются параллельной работой программы.

## 8. Заключение
Были реализованы последовательная (**SEQ**) и параллельная (**MPI**) версия алгоритма. Также были проведены сравнения производительности данных реализаций, исходя из результатов, можем судить об успешном распараллеливании программы. Запуск на 4-х процессах ещё увеличил ускорение, получаемое на 2-х процессах. Из этого можем высказать предположение, что ресурс параллелизма ещё не исчерпан, вполне возможно получить большее ускорение на большем числе процессов. 

## 9. Источники
https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi