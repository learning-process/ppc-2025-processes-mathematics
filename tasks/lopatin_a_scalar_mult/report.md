# Скалярное произведение векторов

- Студент: Лопатин Артём Алексеевич, группа 3823Б1ПМоп3
- Технология: SEQ, MPI
- Вариант: 9

## 1. Введение
В настоящее время в профильных областях широко используются кластерные системы, упрощенно представляющие собой набор вычислительных узлов с индивидуальной памятью. Написание программ под такие системы (ввиду их специфики) требует организации межузловой коммуникации и синхронизации вычислений.

Средства межпроцессового взаимодействия описывает стандарт MPI (*Message Passing Interface* - интерфейс передачи сообщений). В учебных целях с использованием библиотеки (программной реализации стандарта) MPI была решена задача скалярного произведения векторов. Следует отметить, что задача носит ознакомительный характер, поскольку вычислительная трудоемкость алгоритма низкая, и накладные расходы MPI (организация контекста коммуникации, рассылка и сбор данных), вероятно, станут узким местом приложения. Существенного ускорения от параллельной реализации выбранного алгоритма на процессах не ожидается.

## 2. Постановка задачи
Решается задача поиска скалярного произведения двух векторов фиксированной размерности, заданных вещественными координатами.

**Входные данные:** пара векторов `std::pair<std::vector<double>, std::vector<double>>`.  
**Выходные данные:** результат скалярного произведения - число типа `double`.

**Требования:**
1. Реализация *последовательной* (**SEQ**) и *параллельной* (**MPI**) версий алгоритма.
2. Покрытие кода *функциональными* и *performance*-тестами (**gtest**).
3. Соблюдение заданной структуры репозитория и тестового интерфейса.

## 3. Описание базового алгоритма (последовательного)
Базовый алгоритм поиска скалярного произведения тривиален: в цикле по `i = 0..n-1`, где `n` - размерность векторов, производится выборка i-ых компонент обоих векторов, находится их произведение. Результат произведений аккумулируется в результирующей переменной `res` посредством суммирования.

$$res = \sum_{i=0}^{n-1} a_i\cdot b_i$$

Следует отметить, что перед запуском вычислений входные данные проходят обязательную валидацию, которая заключается в проверке равенства размерностей векторов и их непустоты. В случае неудачи вычислительная часть алгоритма не выполняется.

## 4. Схема распараллеливания
Параллельная реализация выполнена с использованием топологии стандартного коммуникатора `MPI_COMM_WORLD`. В качестве корневого процесса используется процесс с рангом 0. Он выполняет распределение данных и аккумулирует результаты частичных скалярных произведений (полученных на всех процессах).

Параллельный алгоритм включает в себя несколько этапов:
1. Размерность входных данных `n`, принимающая корректное значение только на корневом процессе, рассылается всем процессам с помощью функции `MPI_Bcast`.
2. Данные распределяются равномерно по процессам посредством вызова функции `MPI_Scatter` с размером порции `n / proc_num`, где `proc_num` - число процессов коммуникатора `MPI_COMM_WORLD`, `/` - целочисленное деление. 
3. Каждый процесс вычисляет частичное скалярное произведение и сохраняет результат в локальной переменной.
4. Обработкой "хвоста" (при его наличии) занимается процесс с рангом 0 в отдельном цикле (*remainder loop*).
*Замечание:* п. 4 является опциональным и выполняется тогда и только тогда, когда `n` не делится нацело на `proc_num` (в этом случае остается необработанный остаток размера `n % proc_num` элементов в каждом векторе). 
5. Выполняется аккумуляция локальных результатов путем использования функции `MPI_Reduce` с параметром редуцирования `MPI_SUM`.
6. Результат скалярного произведения рассылается всем процессам с помощью функции `MPI_Bcast` в целях синхронизации данных для прохождения тестов (тестовая инфраструктура требует наличия результата на всех процессах).

## 5. Детали реализации

### 5.1 Общий интерфейс (common/include)
В директории `common/include` находится заголовочный файл `common.hpp`, содержащий определение типов входных (`InType`) и выходных (`OutType`) данных, а также тип входных данных тестовых кейсов (`TestType`; в данном случае строка `std::string`, представляющая имя файла с тестовыми данными) и базовый класс интерфейса (`BaseTask`).

### 5.2 Формат тестовых данных (data)
Директория `data` содержит текстовые файлы `.txt` с маской имени `test_vectors_func_n_<size-of-vectors>`. В файлах записаны данные (первые две строки содержат координаты соответствующих векторов, третья строка - результат скалярного произведения, вычисленный вручную) для *функциональных* тестов (приписка `_ort` в имени файла обозначает ортогональность векторов). Бинарный файл `test_vectors_perf_n_4194304.bin` содержит тестовую информацию для *performance*-тестов в формате `<size-of-vectors == 4194304><first-vector-data><second-vector-data><scalar-prod-res>`.

Координаты векторов в `test_vectors_perf_n_4194304.bin` сгенерированы в отдельном приложении с использованием генератора псевдо-случайных чисел типа *Mersenne Twister* `std::mt19937` по равномерному распределению `std::uniform_real_distribution` в отрезке $[-64.0, 64.0]$. За эталонный принят результат скалярного произведения, вычисленный методом `dot()` из библиотеки `Eigen` (версия 5.0.0).

### 5.3 Интерфейс реализаций алгоритмов (mpi & seq)
В директориях `mpi` и `seq` расположены `.hpp` и `.cpp` файлы соответствующих реализаций. Общий интерфейс включает методы валидации данных `bool ValidationImpl(void)` (алгоритм валидации описан в п. 3 отчета, для `mpi` версии валидация выполняется только на корневом процессе), предобработки данных `bool PreProcessingImpl(void)` (в задаче скалярного произведения предобработка входных данных не требуется, функция обнуляет выходные данные и возвращает успех), вычисления скалярного произведения `bool RunImpl(void)` (алгоритмы MPI и SEQ реализаций описаны в соответствующих пунктах отчета) и постобработки данных `bool PostProcessingImpl(void)` (в задаче скалярного произведения постобработка данных не требуется).

### 5.4 Тестирование (tests)
Директория `tests` содержит функциональные и *performance*-тесты с `main.cpp` реализациями в одноименных папках. Оба типа тестов имеют одинаковую структуру: в `void SetUp(void)` инициализации происходит чтение входных данных из файла, по завершении вычислений методом `bool CheckTestOutputData(OutType& output_data)` производится проверка корректности результата путем сравнения с эталонным значением с поправкой на вычислительную погрешность машинной арифметики (оценки погрешности получены экспериментально для каждого теста).

### 5.5 Управление памятью
Управление памятью осуществляется автоматически средствами контейнера `std::vector`. Динамические указатели с ручным управлением памятью не используются.

## 6. Тестовая конфигурация
- Процессор: AMD Ryzen 5 3500x (Zen 2), 6 cores / 6 threads, 3.6 ГГц, Turbo 4.1 ГГц
- Память: 32 GB DDR4
- Операционная система: Windows 11 version 24H2
- Компилятор: GCC 15.2.0, ключи: -O2
- Реализация MPI: MS-MPI version 10.1.12498.52
- Параметры запуска: `mpiexec -n <count>`
- Данные: векторы размерности `16777216`, сгенерированные в соответствие с п. 5.2. и размещенные на локальном SSD-накопителе

## 7. Экспериментальные результаты

### 7.1 Корректность
Проверка корректности осуществлялась путем сравнения с эталонным значением, в качестве которого был принят результат работы метода `dot()` из библиотеки `Eigen` (версия 5.0.0). Допускалась погрешность машинной арифметики порядка 1e-1.

### 7.2 Производительность

| Mode        | Count | Time, ms | Speedup | Efficiency |
|-------------|-------|----------|---------|------------|
| seq         | 1     | 14       | 1.00    | N/A        |
| mpi         | 2     | 55.1433  | 0.25    | 12.5%      |
| mpi         | 4     | 60.5555  | 0.23    | 5.7%       |
| mpi         | 6     | 53.7454  | 0.26    | 4.3%       |
| mpi         | 8     | 73.6061  | 0.19    | 2.4%       |

Полученные результаты показывают значительное **замедление** MPI-реализации скалярного произведения векторов на машине с общей памятью. Наблюдается явление антимасштабируемости, характерное для задач с низким отношением вычислений к объему коммуникаций. Наилучшая производительность достигнута при использовании 6 процессов (соответствует числу ядер).
Результаты эксперимента подтверждают неэффективность использования MPI в задаче поиска скалярного произведения на машине с общей памятью: накладные расходы MPI превосходят вычислительную нагрузку, что кратно замедляет программу.

## 8. Заключение
В результате работы в учебных целях разработаны последовательная (SEQ) и параллельная (MPI) версии программы, вычисляющей скалярное произведение двух вещественных векторов.
Проведенный в процессе выполнения работы вычислительный эксперимент продемонстрировал избыточность использования алгоритмов MPI в нетрудоемком алгоритме скалярного произведения векторов на машине с общей памятью. Установлено, что накладные расходы MPI (формирование контекста коммуникаций, рассылка и сбор данных, синхронизация процессов) кратно увеличивают время работы приложения. Эффективность параллельной реализации при этом не превосходит 12.5%. Наивысшая производительность была достигнута на 6 процессах, что соответствует числу ядер процессора.

## 9. Источники
1. Microsoft MPI : документация [Электронный ресурс] // Microsoft Learn – URL: https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi (дата обращения: 09.11.2025).

## Приложение
```cpp
// remainder loop for tail (mpi)
if (proc_rank == 0) {
    if (n % proc_num != 0) {
        size_t tail_index = n - (n % proc_num);
        for (size_t i = tail_index; i < n; ++i) {
            total_res += input.first[i] * input.second[i];
        }
    }
}
```