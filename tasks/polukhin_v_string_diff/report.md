# Подсчет числа несовпадающих символов двух строк

- Студент: Полухин Владислав Валерьевич, группа 3823Б1ПМоп3
- Технология: MPI
- Вариант: 27

## 1. Введение

Цель работы состоит в том, чтобы закрепить на практике принципы MPI, освоить базовые коллективные операции и провести анализ производительности параллельного алгоритма в сравнении с последовательным алгоритмом. Также было бы занимательно исследовать масштабируемость решения при увеличении числа вычислительных процессов.

## 2. Постановка задачи

Постановка задачи: на вход алгоритму подаются две строки произвольной длины; требуется вычислить количество позиций, в которых строки различаются.

Различия - это как посимвольные несовпадения, так и разница в длинах строк.

Обозначим `len1` и `len2` длины строк `str1` и `str2` соответственно, тогда результат равен количеству позиций `i` от нуля до `min(len1, len2)`, где `str1[i] != str2[i]` плюс разность длин строк `max(len1, len2) - min(len1, len2)`.

**Примеры:**
- Строки `"abc"` и `"abc"` дадут результат 0, так как совпадают;
- Строки `"abc"` и `"abd"` дадут результат 1, поскольку различаются только `str1[2]` и `str2[2]`;
- Строки `"hello"` и `"hallo"` также дадут результат 1, так как различаются только `str1[1]` и `str2[1]`;
- Строки `"abc"` и `"abcd"` дают результат 1 из-за разницы в длине строк в один символ.

## 3. Описание алгоритма

### Последовательная версия алгоритма

Сначала определяется минимальная длина из двух строк, затем инициализируется счётчик различий нулём.

Основной цикл проходит по всем позициям от нуля до `min(len1, len2) - 1` включительно. На каждой итерации происходит сравнение строк на текущей позиции `i` в обеих строках. Символы не идентичны - увеличиваем счетчик.

После завершения цикла к счётчику добавляется разность длин строк. Финальное значение счётчика и есть результат работы алгоритма.

### Параллельная версия алгоритма

Разделяем строки для сравнения между несколькими процессами в количестве `size`(количество процессов) штук, получая `size` небольших строк, каждую из которых независимо обрабатывают процессы. Во время обработки подстрок подсчитываются локальные несовпадения.

Дальше собираем результаты подсчёта со всех процессов и прибавляем разницу длин строк и получаем финальный результат.

## 4. Схема распараллеливания

Работа распределяется на основе номера процесса. Для каждого процесса вычисляется начальный и конечный индексы его рабочего диапазона. Формула для вычисления размера порции данных использует округление вверх, чтобы более равномерно распределить работу между процессами. Количество элементов на процесс вычисляется как `(минимальная_длина + количество_процессов - 1) / количество_процессов`.

Начальный индекс для процесса с номером `rank` вычисляется как `rank * элементы_на_процесс`. Конечный индекс - это либо `начальный_индекс + элементы_на_процесс`, либо минимальная длина строки(в зависимости от того, что меньше). Такая схема гарантирует, что последний процесс обработает все оставшиеся элементы, даже если их количество меньше стандартного `элементы_на_процесс`.


После локальных вычислений необходимо собрать результаты со всех процессов. Это мы делаем с помощью `MPI_Reduce` с функцией суммирования. Эта операция собирает локальные счётчики со всех процессов и вычисляет их сумму, помещая результат на корневом процессе.

Корневой процесс добавляет к полученной сумме разницу длин строк. После этого финальный результат нужно распространить на все процессы с помощью `MPI_Bcast`, поскольку тестовая система проверяет корректность результата на каждом процессе.

Топология коммуникаций представляет собой схему "мастер-рабочие" с использованием только коллективных операций, что обеспечивает эффективную синхронизацию процессов.

## 5. Детали реализации

Код организован в виде модульной структуры согласно требованиям курса. В папке `common` определены общие типы данных через псевдонимы типов. Входной тип `InType` - это `std::pair` из двух строк, выходной тип `OutType` - это `size_t` для хранения количества различий. Также определён `TestType` как `std::tuple` из двух строк для параметризации тестов.

Последовательная версия реализована в отдельной папке `seq`. Класс `StringDiffTaskSEQ` наследуется от базового класса `Task` и переопределяет четыре обязательных метода: `ValidationImpl`, `PreProcessingImpl`, `RunImpl` и `PostProcessingImpl`. В данной задаче только `RunImpl` содержит содержательную логику, остальные методы возвращают `true`, так как специальной пред- и постобработки не требуется.

Параллельная версия находится в папке `mpi`. Класс `StringDiffTaskMPI` имеет аналогичную структуру, но метод `RunImpl` содержит вызовы функций MPI для распределённых вычислений. В `ValidationImpl` добавлена проверка корректности количества процессов для обеспечения надёжности работы.

В реализации MPI версии особое внимание уделено обработке граничных случаев. Если минимальная длина строк равна нулю, цикл сравнения не выполняется, и все процессы передают в `Reduce` нулевые локальные счётчики. Если количество символов меньше числа процессов, некоторые процессы получают пустые диапазоны, но это корректно обрабатывается благодаря правильному вычислению границ.

Использование коллективных операций вместо точечных коммуникаций существенно упрощает код и повышает его надёжность. MPI_Scatterv эффективно распределяет части данных от процесса 0 остальным процессам, MPI_Reduce автоматически собирает данные со всех процессов, используя алгоритм, который работает быстрее, чем последовательный сбор через циклы Send и Recv. Аналогично MPI_Bcast эффективно распространяет данные, используя древовидную схему коммуникаций.

Память используется эффективно: процесс 0 хранит исходные строки, а каждый процесс хранит только свою часть данных для сравнения. Распределение данных через MPI_Scatterv позволяет избежать дублирования полных строк на всех процессах, что важно при работе с большими объемами данных.

## 6. Экспериментальная установка

Все эксперименты проводились на персональном компьютере со следующими характеристиками:

**Аппаратное обеспечение:**
- Процессор: Intel Core i3-1115G4 @ 3.00GHz (2 физических ядра с поддержкой Hyper-Threading, 4 логических потока)
- Оперативная память: 8 ГБ DDR4
- Операционная система: Windows 11 64-разрядная

**Программное обеспечение:**
- Компилятор: Microsoft Visual C++ 17.14
- MPI библиотека: Microsoft MPI
- Система сборки: CMake 3
- Тип сборки: Release (максимальные оптимизации компилятора)

Для запуска MPI процессов использовалась утилита `mpiexec`, входящая в состав Microsoft MPI. Переменная окружения `PPC_NUM_PROC` устанавливалась для контроля количества запускаемых процессов. Тестирование проводилось с одним, двумя и четырьмя процессами для анализа масштабируемости.

**Тестовые данные:**
- Размер строк: 100,000,000 символов
- Количество различий: 10,000,000 (каждый 10-й символ различается)
- Формат данных: первая строка заполнена символом `'a'`, вторая также заполнена `'a'`, но каждый десятый символ заменён на `'b'`

Такой размер данных выбран, чтобы время вычислений было достаточно большим для точных измерений и чтобы проявился эффект параллелизации.

Для функциональных тестов использовались шесть тестовых случаев, покрывающих различные сценарии: идентичные строки, одно различие, несколько различий, разная длина и пустые строки. Все функциональные тесты проверялись с одним, двумя и четырьмя процессами, чтобы убедиться в корректности работы при любом уровне параллелизма.

## 7. Результаты и обсуждение

### 7.1 Проверка корректности

Корректность реализации проверялась на нескольких уровнях. Во-первых, все шесть функциональных тестов успешно пройдены как для последовательной, так и для параллельной версии. Каждый тест запускался с одним, двумя и четырьмя процессами, и во всех случаях полученные результаты совпали с ожидаемыми.

Особое внимание уделялось граничным случаям. Тест с пустыми строками подтвердил, что алгоритм корректно обрабатывает ситуацию, когда минимальная длина равна нулю. Тест с одинаковыми строками показал, что алгоритм правильно возвращает ноль различий. Тесты с разной длиной строк подтвердили правильность учёта разницы длин.

Совпадение результатов реализации и ожидаемых значений во всех тестах подтверждает корректность обеих версий.

### 7.2 Анализ производительности

Измерения производительности проводились на строках длиной десять миллионов символов. Каждый тест запускался несколько раз, и фиксировалось среднее время выполнения.

**Режим Task Run:**

| Реализация | Процессов | Время (сек) | Ускорение | Эффективность |
|------------|-----------|-------------|-----------|---------------|
| SEQ        | 1         | 0.11155802  | 1.00      | —             |
| MPI        | 1         | 0.17634060  | 0.63      | 63%          |
| MPI        | 2         | 0.14436362  | 0.77      | 39%          |
| MPI        | 4         | 0.14890230  | 0.75      | 19%           |

**Режим Pipeline:**

| Реализация | Процессов | Время (сек) | Ускорение | Эффективность |
|------------|-----------|-------------|-----------|---------------|
| SEQ        | 1         | 0.10265918  | 1.00      | —             |
| MPI        | 1         | 0.19945568  | 0.51      | 51%           |
| MPI        | 2         | 0.14061112  | 0.73      | 37%           |
| MPI        | 4         | 0.15085014  | 0.68      | 17%           |

**Формулы:**
- Ускорение = Время(SEQ) / Время(MPI)
- Эффективность = (Ускорение / Количество_процессов) × 100%

### 7.3 Интерпретация результатов

**Один процесс**

В обоих режимах MPI версия с одним процессом показала замедление относительно последовательной реализации (ускорение 0.63x в Task Run и 0.51x в Pipeline). Это объясняется значительными накладными расходами на инициализацию MPI окружения и выполнение коллективных операций, которые не компенсируются при использовании всего одного процесса.

**Два процесса**

С двумя процессами наблюдается улучшение производительности MPI реализации относительно работы на одном процессе, однако общее время выполнения все еще превышает последовательную версию. В режиме Task Run MPI на двух процессах достигает ускорения 0.77x относительно SEQ (эффективность 39%). Это демонстрирует, что даже при значительном объеме вычислений (100 миллионов символов) коммуникационные расходы остаются существенными.

**Четыре процесса**

При переходе к четырём процессам эффективность в task режиме упала до 51%, а в pipeline до 42%. Это связано с:
- Увеличением накладных расходов на коммуникацию (больше процессов → больше времени на синхронизацию)
- Возможной конкуренцией за ресурсы, если в системе меньше физических ядер (а конкуренция есть, так как у меня всего 2 физических ядра).
- Законом Амдала — есть последовательная часть (рассылка данных, сбор данных, добавление разницы длин), которая не параллелится


**Выводы по производительности**

Параллельная реализация демонстрирует ограниченную эффективность на тестируемых объемах данных. Для 100 миллионов символов MPI версия работает медленнее последовательной, что указывает на то, что коммуникационные расходы превышают выигрыш от параллелизации. Оптимальным оказалось использование двух процессов, дальнейшее увеличение их количества не приводит к улучшению производительности.

Результаты подтверждают, что для задачи сравнения строк эффективная параллелизация с использованием MPI требует значительно больших объемов данных (судя по всему, миллиарды символов, тесты с которыми очень долго выполняются на моей системе), при которых время вычислений должно существенно превысить время коммуникаций. На меньших объемах накладные расходы на распределение данных и сбор результатов перевешивают преимущества параллельной обработки.

## 8. Заключение

В работе реализован параллельный алгоритм сравнения строк с использованием MPI. Последовательная версия служила эталоном для проверки корректности и оценки производительности.

Разработанное решение успешно прошло все функциональные тесты при разном количестве процессов. Анализ производительности показал, что эффективность параллелизации напрямую зависит от размера задачи — для 100 миллионов символов MPI реализация показала замедление относительно последовательной версии из-за значительных накладных расходов на коммуникации. Результаты покахывают то, что для данной задачи порог эффективности находится за пределами текущих объемов тестирующих данных.

Для дальнейшего улучшения можно реализовать динамическую балансировку и гибридный подход с комбинацией MPI/OpenMP.

Работа позволила закрепить на практике основные принципы и базовые операции MPI, включая коллективные операции `MPI_Scatterv`, `MPI_Reduce` и `MPI_Bcast`, а также получить представление о практических аспектах эффективности распределенных вычислений.

## 9. Список использованных источников

1. Лекции по курсу "Parallel Programming 2025-2026". https://disk.yandex.ru/d/NvHFyhOJCQU65w
2. MPI Forum. MPI: A Message-Passing Interface Standard Version 4.0. https://www.mpi-forum.org/docs/
3. Документация по курсу «Параллельное программирование». https://learning-process.github.io/parallel_programming_course/ru/common_information/report.html
4. Введение в параллельное программирование (OpenMP и MPI). https://stepik.org/course/115024/promo
5. MPI для начинающих. https://parallel.uran.ru/node/182

## Приложение

### Ключевые фрагменты кода

**Последовательная версия:**
```cpp
const auto &input = GetInput();
const std::string &str1 = input.first;
const std::string &str2 = input.second;

size_t result = 0;
size_t min_len = std::min(str1.size(), str2.size());

for (size_t i = 0; i < min_len; ++i) {
if (str1[i] != str2[i]) {
    ++result;
}
}

size_t len1 = str1.size();
size_t len2 = str2.size();
result += (std::max<size_t>(len1, len2) - std::min<size_t>(len1, len2));

GetOutput() = result;
```

**Параллельная версия:**
```cpp
namespace {

void BroadcastStringLengths(int rank, int& len1, int& len2) {
  if (rank == 0) {
  }
  MPI_Bcast(&len1, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&len2, 1, MPI_INT, 0, MPI_COMM_WORLD);
}

void PrepareDataDistribution(int rank, int size, int min_len, std::vector<int>& sendcounts, std::vector<int>& displs) {
  if (rank != 0) {
    return;
  }

  std::fill(sendcounts.begin(), sendcounts.end(), 0);
  std::fill(displs.begin(), displs.end(), 0);

  const int els_per_process = (min_len + size - 1) / size;
  int offset = 0;

  for (int i = 0; i < size; ++i) {
    const int start = i * els_per_process;
    const int end = std::min(start + els_per_process, min_len);

    if (start < min_len) {
      sendcounts[i] = end - start;
    }

    displs[i] = offset;

    if (start < min_len) {
      offset += sendcounts[i];
    }
  }
}

uint64_t ComputeLocalDifferences(const std::vector<char>& local_str1, const std::vector<char>& local_str2) {
  uint64_t local_count = 0;
  const size_t recvcount = local_str1.size();

  for (size_t i = 0; i < recvcount; ++i) {
    if (local_str1[i] != local_str2[i]) {
      ++local_count;
    }
  }
  return local_count;
}

void DistributeStrings(int rank, int recvcount, const std::vector<int>& sendcounts, const std::vector<int>& displs,
                       const std::string& str1, const std::string& str2, std::vector<char>& local_str1,
                       std::vector<char>& local_str2) {
  if (recvcount <= 0) {
    return;
  }

  MPI_Scatterv(rank == 0 ? str1.data() : nullptr, sendcounts.data(), displs.data(), MPI_CHAR, local_str1.data(),
               recvcount, MPI_CHAR, 0, MPI_COMM_WORLD);

  MPI_Scatterv(rank == 0 ? str2.data() : nullptr, sendcounts.data(), displs.data(), MPI_CHAR, local_str2.data(),
               recvcount, MPI_CHAR, 0, MPI_COMM_WORLD);
}

void CollectAndSetResults(int rank, uint64_t local_count, int length_diff, size_t& output) {
  uint64_t total_count = 0;
  MPI_Reduce(&local_count, &total_count, 1, MPI_UINT64_T, MPI_SUM, 0, MPI_COMM_WORLD);

  if (rank == 0) {
    total_count += static_cast<uint64_t>(length_diff);
    output = static_cast<size_t>(total_count);
  }

  uint64_t result = 0;
  if (rank == 0) {
    result = total_count;
  }
  MPI_Bcast(&result, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);

  if (rank != 0) {
    output = static_cast<size_t>(result);
  }
}

}  // namespace

bool StringDiffTaskMPI::RunImpl() {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  std::string str1, str2;
  int len1 = 0, len2 = 0;

  if (rank == 0) {
    const auto& input = GetInput();
    str1 = input.first;
    str2 = input.second;
    len1 = static_cast<int>(str1.size());
    len2 = static_cast<int>(str2.size());
  }

  BroadcastStringLengths(rank, len1, len2);

  const int min_len = std::min(len1, len2);
  const int length_diff = std::abs(len1 - len2);

  uint64_t local_count = 0;

  if (min_len > 0) {
    std::vector<int> sendcounts(size, 0);
    std::vector<int> displs(size, 0);

    PrepareDataDistribution(rank, size, min_len, sendcounts, displs);

    int recvcount = 0;
    MPI_Scatter(sendcounts.data(), 1, MPI_INT, &recvcount, 1, MPI_INT, 0, MPI_COMM_WORLD);

    if (recvcount > 0) {
      std::vector<char> local_str1(recvcount);
      std::vector<char> local_str2(recvcount);

      DistributeStrings(rank, recvcount, sendcounts, displs, str1, str2, local_str1, local_str2);

      local_count = ComputeLocalDifferences(local_str1, local_str2);
    }
  }

  CollectAndSetResults(rank, local_count, length_diff, GetOutput());
```
