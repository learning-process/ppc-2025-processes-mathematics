# Подсчет числа несовпадающих символов двух строк

- Студент: Полухин Владислав Валерьевич, группа 3823Б1ПМоп3
- Технология: MPI
- Вариант: 27

## 1. Введение

Цель работы состоит в том, чтобы закрепить на практике принципы MPI, освоить базовые коллективные операции и провести анализ производительности параллельного алгоритма в сравнении с последовательным алгоритмом. Также было бы занимательно исследовать масштабируемость решения при увеличении числа вычислительных процессов.

## 2. Постановка задачи

Постановка задачи: на вход алгоритму подаются две строки произвольной длины; требуется вычислить количество позиций, в которых строки различаются.

Различия - это как посимвольные несовпадения, так и разница в длинах строк.

Обозначим `len1` и `len2` длины строк `str1` и `str2` соответственно, тогда результат равен количеству позиций `i` от нуля до `min(len1, len2)`, где `str1[i] != str2[i]` плюс разность длин строк `max(len1, len2) - min(len1, len2)`.

**Примеры:**
- Строки `"abc"` и `"abc"` дадут результат 0, так как совпадают;
- Строки `"abc"` и `"abd"` дадут результат 1, поскольку различаются только `str1[2]` и `str2[2]`;
- Строки `"hello"` и `"hallo"` также дадут результат 1, так как различаются только `str1[1]` и `str2[1]`;
- Строки `"abc"` и `"abcd"` дают результат 1 из-за разницы в длине строк в один символ.

## 3. Описание алгоритма

### Последовательная версия алгоритма

Сначала определяется минимальная длина из двух строк, затем инициализируется счётчик различий нулём.

Основной цикл проходит по всем позициям от нуля до `min(len1, len2) - 1` включительно. На каждой итерации происходит сравнение строк на текущей позиции `i` в обеих строках. Символы не идентичны - увеличиваем счетчик.

После завершения цикла к счётчику добавляется разность длин строк. Финальное значение счётчика и есть результат работы алгоритма.

### Параллельная версия алгоритма

Разделяем строки для сравнения между несколькими процессами в количестве `size`(количество процессов) штук, получая `size` небольших строк, каждую из которых независимо обрабатывают процессы. Во время обработки подстрок подсчитываются локальные несовпадения.

Дальше собираем результаты подсчёта со всех процессов и прибавляем разницу длин строк и получаем финальный результат.

## 4. Схема распараллеливания

Работа распределяется на основе номера процесса. Для каждого процесса вычисляется начальный и конечный индексы его рабочего диапазона. Формула для вычисления размера порции данных использует округление вверх, чтобы более равномерно распределить работу между процессами. Количество элементов на процесс вычисляется как `(минимальная_длина + количество_процессов - 1) / количество_процессов`.

Начальный индекс для процесса с номером `rank` вычисляется как `rank * элементы_на_процесс`. Конечный индекс - это либо `начальный_индекс + элементы_на_процесс`, либо минимальная длина строки(в зависимости от того, что меньше). Такая схема гарантирует, что последний процесс обработает все оставшиеся элементы, даже если их количество меньше стандартного `элементы_на_процесс`.


После локальных вычислений необходимо собрать результаты со всех процессов. Это мы делаем с помощью `MPI_Reduce` с функцией суммирования. Эта операция собирает локальные счётчики со всех процессов и вычисляет их сумму, помещая результат на корневом процессе.

Корневой процесс добавляет к полученной сумме разницу длин строк. После этого финальный результат нужно распространить на все процессы с помощью `MPI_Bcast`, поскольку тестовая система проверяет корректность результата на каждом процессе.

Топология коммуникаций представляет собой схему "мастер-рабочие" с использованием только коллективных операций, что обеспечивает эффективную синхронизацию процессов.

## 5. Детали реализации

Код организован в виде модульной структуры согласно требованиям курса. В папке `common` определены общие типы данных через псевдонимы типов. Входной тип `InType` - это `std::pair` из двух строк, выходной тип `OutType` - это `size_t` для хранения количества различий. Также определён `TestType` как `std::tuple` из двух строк для параметризации тестов.

Последовательная версия реализована в отдельной папке `seq`. Класс `StringDiffTaskSEQ` наследуется от базового класса `Task` и переопределяет четыре обязательных метода: `ValidationImpl`, `PreProcessingImpl`, `RunImpl` и `PostProcessingImpl`. В данной задаче только `RunImpl` содержит содержательную логику, остальные методы возвращают `true`, так как специальной пред- и постобработки не требуется.

Параллельная версия находится в папке `mpi`. Класс `StringDiffTaskMPI` имеет аналогичную структуру, но метод `RunImpl` содержит вызовы функций MPI для распределённых вычислений. В `ValidationImpl` добавлена проверка корректности количества процессов для обеспечения надёжности работы.

В реализации MPI версии особое внимание уделено обработке граничных случаев. Если минимальная длина строк равна нулю, цикл сравнения не выполняется, и все процессы передают в `Reduce` нулевые локальные счётчики. Если количество символов меньше числа процессов, некоторые процессы получают пустые диапазоны, но это корректно обрабатывается благодаря правильному вычислению границ.

Использование коллективных операций вместо точечных коммуникаций существенно упрощает код и повышает его надёжность. `MPI_Reduce` автоматически собирает данные со всех процессов, используя внутренне оптимизированный алгоритм, который может работать быстрее, чем последовательный сбор через циклы `Send` и `Recv`. Аналогично `MPI_Bcast` эффективно распространяет данные, используя древовидную схему коммуникаций.

Память используется эффективно: каждый процесс хранит только копии входных строк и небольшое количество переменных для вычислений. Дублирование строк на всех процессах допустимо для данного размера задачи, но при работе с очень большими данными потребовалась бы более сложная схема распределённого хранения.

## 6. Экспериментальная установка

Все эксперименты проводились на персональном компьютере со следующими характеристиками:

**Аппаратное обеспечение:**
- Процессор: Intel Core i3-1115G4 @ 3.00GHz (2 физических ядра с поддержкой Hyper-Threading, 4 логических потока)
- Оперативная память: 8 ГБ DDR4
- Операционная система: Windows 11 64-разрядная

**Программное обеспечение:**
- Компилятор: Microsoft Visual C++ 17.14
- MPI библиотека: Microsoft MPI
- Система сборки: CMake 3
- Тип сборки: Release (максимальные оптимизации компилятора)

Для запуска MPI процессов использовалась утилита `mpiexec`, входящая в состав Microsoft MPI. Переменная окружения `PPC_NUM_PROC` устанавливалась для контроля количества запускаемых процессов. Тестирование проводилось с одним, двумя и четырьмя процессами для анализа масштабируемости.

**Тестовые данные:**
- Размер строк: 10,000,000 символов
- Количество различий: 1,000,000 (каждый 10-й символ различается)
- Формат данных: первая строка заполнена символом `'a'`, вторая также заполнена `'a'`, но каждый десятый символ заменён на `'b'`

Такой размер данных выбран, чтобы время вычислений было достаточно большим для точных измерений и чтобы проявился эффект параллелизации.

Для функциональных тестов использовались шесть тестовых случаев, покрывающих различные сценарии: идентичные строки, одно различие, несколько различий, разная длина и пустые строки. Все функциональные тесты проверялись с одним, двумя и четырьмя процессами, чтобы убедиться в корректности работы при любом уровне параллелизма.

## 7. Результаты и обсуждение

### 7.1 Проверка корректности

Корректность реализации проверялась на нескольких уровнях. Во-первых, все шесть функциональных тестов успешно пройдены как для последовательной, так и для параллельной версии. Каждый тест запускался с одним, двумя и четырьмя процессами, и во всех случаях полученные результаты совпали с ожидаемыми.

Особое внимание уделялось граничным случаям. Тест с пустыми строками подтвердил, что алгоритм корректно обрабатывает ситуацию, когда минимальная длина равна нулю. Тест с одинаковыми строками показал, что алгоритм правильно возвращает ноль различий. Тесты с разной длиной строк подтвердили правильность учёта разницы длин.

Совпадение результатов реализации и ожидаемых значений во всех тестах подтверждает корректность обеих версий.

### 7.2 Анализ производительности

Измерения производительности проводились на строках длиной десять миллионов символов. Каждый тест запускался несколько раз, и фиксировалось среднее время выполнения.

**Режим Task Run:**

| Реализация | Процессов | Время (сек) | Ускорение | Эффективность |
|------------|-----------|-------------|-----------|---------------|
| SEQ        | 1         | 0.00888772  | 1.00      | —             |
| MPI        | 1         | 0.00780446  | 1.13      | 113%          |
| MPI        | 2         | 0.00418256  | 2.12      | 106%          |
| MPI        | 4         | 0.00435498  | 2.16      | 51%           |

**Режим Pipeline:**

| Реализация | Процессов | Время (сек) | Ускорение | Эффективность |
|------------|-----------|-------------|-----------|---------------|
| SEQ        | 1         | 0.00969772  | 1.00      | —             |
| MPI        | 1         | 0.01018988  | 0.95      | 95%           |
| MPI        | 2         | 0.00717252  | 1.35      | 68%           |
| MPI        | 4         | 0.00582360  | 1.67      | 42%           |

**Формулы:**
- Ускорение = Время(SEQ) / Время(MPI)
- Эффективность = (Ускорение / Количество_процессов) × 100%

### 7.3 Интерпретация результатов

**Один процесс**

В режиме task run MPI версия с одним процессом показала ускорение 1.13x, что может объясняться несколькими причинами:
- Компилятор мог применить разные оптимизации для MPI кода
- Возможны эффекты кэша из-за разного порядка доступа к памяти
- MPI библиотека могла использовать оптимизированные низкоуровневые функции

В режиме pipeline наблюдается небольшое замедление (0.95x), что связано с дополнительными накладными расходами на инициализацию MPI окружения.

**Два процесса**

С двумя процессами получены наилучшие результаты — ускорение 2.12x в task режиме и 1.35x в pipeline. Суперлинейное ускорение в task режиме (эффективность 106%) можно объяснить эффектами кэша процессора: когда каждый процесс обрабатывает 5 миллионов символов вместо 10, данные лучше помещаются в L2/L3 кэш, что снижает количество обращений к оперативной памяти, которая является медленной.

**Четыре процесса**

При переходе к четырём процессам эффективность в task режиме упала до 51%, а в pipeline до 42%. Это связано с:
- Увеличением накладных расходов на коммуникацию (больше процессов → больше времени на синхронизацию)
- Возможной конкуренцией за ресурсы, если в системе меньше физических ядер (у меня их всего 2)
- Законом Амдала — есть последовательная часть (добавление разницы длин), которая не параллелится

Тем не менее, ускорение 2.16x в task режиме и 1.67x в pipeline — это хорошие результаты, показывающие, что алгоритм эффективно масштабируется.

**Выводы по производительности**

Параллельная реализация показала свою эффективность на больших объёмах данных. Для 10 миллионов символов получено ускорение до 2.12 раз. Оптимальным оказалось использование двух процессов — дальнейшее увеличение даёт меньший прирост из-за накладных расходов.

Замечу, что в более ранних экспериментах с меньшими строками (10 тысяч символов) MPI версия была медленнее последовательной из-за влияния коммуникационных расходов, которые перевешивали преимущества от параллелизации. Это подтверждает важность выбора правильного размера задачи для эффективного параллелизма.

## 8. Заключение

В работе реализован параллельный алгоритм сравнения строк с использованием MPI. Последовательная версия служила эталоном для проверки корректности и оценки производительности.

Разработанное решение успешно прошло все функциональные тесты при разном количестве процессов. Анализ производительности показал, что эффективность параллелизации напрямую зависит от размера задачи — ощутимый выигрыш проявляется только на больших объемах данных, тогда как для малых строк накладные расходы на коммуникации превышает преимущества распараллеливания.

Для дальнейшего улучшения можно реализовать динамическую балансировку и гибридный подход с комбинацией MPI/OpenMP.

Работа позволила закрепить на практике основные принципы и базовые операции MPI.

## 9. Список использованных источников

1. Лекции по курсу "Parallel Programming 2025-2026". https://disk.yandex.ru/d/NvHFyhOJCQU65w
2. MPI Forum. MPI: A Message-Passing Interface Standard Version 4.0. https://www.mpi-forum.org/docs/
3. Документация по курсу «Параллельное программирование». https://learning-process.github.io/parallel_programming_course/ru/common_information/report.html
4. Введение в параллельное программирование (OpenMP и MPI). https://stepik.org/course/115024/promo
5. MPI для начинающих. https://parallel.uran.ru/node/182

## Приложение

### Ключевые фрагменты кода

**Последовательная версия:**
```cpp
const auto &input = GetInput();
const std::string &str1 = input.first;
const std::string &str2 = input.second;

size_t result = 0;
size_t min_len = std::min(str1.size(), str2.size());

for (size_t i = 0; i < min_len; ++i) {
if (str1[i] != str2[i]) {
    ++result;
}
}

size_t len1 = str1.size();
size_t len2 = str2.size();
result += (std::max<size_t>(len1, len2) - std::min<size_t>(len1, len2));

GetOutput() = result;
```

**Параллельная версия:**
```cpp
int rank = 0;
  int size = 0;

  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  const auto &input = GetInput();
  const std::string &str1 = input.first;
  const std::string &str2 = input.second;

  int min_len = static_cast<int>(std::min(str1.size(), str2.size()));
  size_t len1 = str1.size();
  size_t len2 = str2.size();
  size_t length_diff = std::max<size_t>(len1, len2) - std::min<size_t>(len1, len2);

  size_t local_count = 0;

  if (min_len > 0) {
    size_t els_per_process = (min_len + size - 1) / size;
    size_t start = rank * els_per_process;
    size_t end = std::min<size_t>(start + els_per_process, min_len);

    for (size_t i = start; i < end; ++i) {
      if (str1[i] != str2[i]) {
        ++local_count;
      }
    }
  }

  size_t total_count = 0;
  MPI_Reduce(&local_count, &total_count, 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);

  if (rank == 0) {
    total_count += length_diff;
    GetOutput() = total_count;
  }

  size_t result = (rank == 0) ? GetOutput() : 0;
  MPI_Bcast(&result, 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);

  if (rank != 0) {
    GetOutput() = result;
  }
```
