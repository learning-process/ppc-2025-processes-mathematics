# Подсчет числа несовпадающих символов двух строк

- Студент: Полухин Владислав Валерьевич, группа 3823Б1ПМоп3
- Технология: MPI
- Вариант: 27

## 1. Введение

Сравнение строк является одной из базовых операций в программировании, которая находит применение в самых разных областях от обработки текстов до анализа ДНК. Эта работа заключается в разработке и анализе MPI реализации алгоритма подсчёта несовпадающих символов между двумя строками.

Цель работы состоит в том, чтобы закрепить на практике принципы параллельных вычислений на кластерных системах, освоить базовые коллективные операции MPI и провести анализ производительности параллельного алгоритма в сравнении с последовательной версией реализации. Также вызывает интерес исследовать масштабируемость решения при увеличении числа вычислительных процессов.

## 2. Постановка задачи

Постановка задачи выглядит следующим образом: на вход алгоритму подаются две строки произвольной длины, которые могут содержать любые печатные символы из таблицы ASCII; требуется вычислить общее количество позиций, в которых строки различаются.

Алгоритм должен учитывать два типа различий:
- **Первый тип** - это посимвольные несовпадения на соответствующих позициях строк до конца более короткой из них
- **Второй тип** - это разница в длинах строк, поскольку в более длинной строке есть некоторое количество символов (уже не важно, каких именно), которым ничего не соответствует в более короткой

Результат можно выразить как сумму двух компонент. Если обозначить длины строк как `len1` и `len2`, то результат равен количеству позиций `i` от нуля до `min(len1, len2)`, где `str1[i] ≠ str2[i]`, плюс значение разности `max(len1, len2) - min(len1, len2)`.

**Примеры:**
- Строки `"abc"` и `"abc"` дают результат 0, так как они полностью идентичны
- Строки `"abc"` и `"abd"` дают результат 1, поскольку различаются только `str1[2]` и `str2[2]`
- Строки `"hello"` и `"hallo"` также дают 1, так как различаются только `str1[1]` и `str2[1]`
- Строки `"abc"` и `"abcd"` дают результат 1 из-за разницы в длине на один символ

## 3. Описание алгоритма

### Последовательная версия алгоритма

Сначала определяется минимальная длина из двух строк, что позволяет избежать выхода за границы массива при сравнении. Затем инициализируется счётчик различий нулём.

Основной цикл проходит по всем позициям от нуля до `min(len1, len2) - 1`. На каждой итерации происходит сравнение символов на текущей позиции в обеих строках. Если символы не совпадают, счётчик увеличивается на единицу.

После завершения цикла к счётчику добавляется разность длин строк. Финальное значение счётчика и является результатом работы алгоритма.

### Параллельная версия алгоритма

Основная идея заключается в том, чтобы разделить диапазон индексов строк для сравнения между несколькими вычислительными процессами в количестве `n`, получая `n` более мелких диапазонов, каждый из которых независимо обрабатывает свою часть и подсчитывает локальные различия.

Логика подсчёта различий в каждом из процессов аналогична описанной выше (как в последовательной версии), т.е. проходим в каждом из процессов по позициям доставшегося диапазона и сравниваем символы на текущей позиции в обеих строках; если символы не совпадают, увеличиваем счётчик на единицу.

Дальше собираем результаты подсчёта со всех процессов и прибавляем разницу длин строк.

## 4. Схема распараллеливания

Распределение работы осуществляется на основе номера процесса. Для каждого процесса вычисляется начальный и конечный индексы его рабочего диапазона. Формула для вычисления размера порции данных использует округление вверх, чтобы гарантировать покрытие всех элементов. Конкретно, количество элементов на процесс вычисляется как `(минимальная_длина + количество_процессов - 1) / количество_процессов`.

Начальный индекс для процесса с номером `rank` вычисляется как `rank * элементов_на_процесс`. Конечный индекс - это либо `начальный + элементов_на_процесс`, либо минимальная длина строки, в зависимости от того, что меньше. Такая схема гарантирует, что последний процесс обработает все оставшиеся элементы, даже если их количество меньше стандартного размера порции.

После локальных вычислений необходимо собрать результаты со всех процессов. Для этого используется коллективная операция `MPI_Reduce` с функцией суммирования. Эта операция собирает локальные счётчики со всех процессов и вычисляет их сумму, помещая результат на корневом процессе с номером ноль.

Корневой процесс добавляет к полученной сумме разницу длин строк. После этого финальный результат необходимо распространить на все процессы, поскольку тестовая система проверяет корректность результата на каждом процессе. Поэтому используется коллективная операция `MPI_Bcast`, которая рассылает данные от корневого процесса всем остальным.

Топология коммуникаций представляет собой схему "мастер-рабочие" с использованием только коллективных операций. Это гарантирует отсутствие взаимных блокировок и обеспечивает эффективную синхронизацию всех процессов.

## 5. Детали реализации

Код организован в виде модульной структуры согласно требованиям курса. В папке `common` определены общие типы данных через псевдонимы типов. Входной тип `InType` - это `std::pair` из двух строк, выходной тип `OutType` - это `size_t` для хранения количества различий. Также определён `TestType` как `std::tuple` из двух строк для параметризации тестов.

Последовательная версия реализована в отдельной папке `seq`. Класс `StringDiffTaskSEQ` наследуется от базового класса `Task` и переопределяет четыре обязательных метода: `ValidationImpl`, `PreProcessingImpl`, `RunImpl` и `PostProcessingImpl`. В данной задаче только `RunImpl` содержит содержательную логику, остальные методы возвращают `true`, так как специальной пред- и постобработки не требуется.

Параллельная версия находится в папке `mpi`. Класс `StringDiffTaskMPI` имеет аналогичную структуру, но метод `RunImpl` содержит вызовы функций MPI для распределённых вычислений. В `ValidationImpl` добавлена проверка корректности количества процессов для обеспечения надёжности работы.

В реализации MPI версии особое внимание уделено обработке граничных случаев. Если минимальная длина строк равна нулю, цикл сравнения не выполняется, и все процессы передают в `Reduce` нулевые локальные счётчики. Если количество символов меньше числа процессов, некоторые процессы получают пустые диапазоны, но это корректно обрабатывается благодаря правильному вычислению границ.

Использование коллективных операций вместо точечных коммуникаций существенно упрощает код и повышает его надёжность. `MPI_Reduce` автоматически собирает данные со всех процессов, используя внутренне оптимизированный алгоритм, который может работать быстрее, чем последовательный сбор через циклы `Send` и `Recv`. Аналогично `MPI_Bcast` эффективно распространяет данные, используя древовидную схему коммуникаций.

Память используется эффективно: каждый процесс хранит только копии входных строк и небольшое количество переменных для вычислений. Дублирование строк на всех процессах допустимо для данного размера задачи, но при работе с очень большими данными потребовалась бы более сложная схема распределённого хранения.

## 6. Экспериментальная установка

Все эксперименты проводились на персональном компьютере со следующими характеристиками:

**Аппаратное обеспечение:**
- Процессор: Intel Core i3-1115G4 @ 3.00GHz (2 физических ядра с поддержкой Hyper-Threading, 4 логических потока)
- Оперативная память: 8 ГБ DDR4
- Операционная система: Windows 11 64-разрядная

**Программное обеспечение:**
- Компилятор: Microsoft Visual C++ 17.14
- MPI библиотека: Microsoft MPI
- Система сборки: CMake 3
- Тип сборки: Release (максимальные оптимизации компилятора)

Для запуска MPI процессов использовалась утилита `mpiexec`, входящая в состав Microsoft MPI. Переменная окружения `PPC_NUM_PROC` устанавливалась для контроля количества запускаемых процессов. Тестирование проводилось с одним, двумя и четырьмя процессами для анализа масштабируемости.

**Тестовые данные:**
- Размер строк: 10,000,000 символов
- Количество различий: 1,000,000 (каждый 10-й символ различается)
- Формат данных: первая строка заполнена символом `'a'`, вторая также заполнена `'a'`, но каждый десятый символ заменён на `'b'`

Такой размер данных выбран, чтобы время вычислений было достаточно большим для точных измерений и чтобы проявился эффект параллелизации.

Для функциональных тестов использовались шесть тестовых случаев, покрывающих различные сценарии: идентичные строки, одно различие, несколько различий, разная длина и пустые строки. Все функциональные тесты проверялись с одним, двумя и четырьмя процессами, чтобы убедиться в корректности работы при любом уровне параллелизма.

## 7. Результаты и обсуждение

### 7.1 Проверка корректности

Корректность реализации проверялась на нескольких уровнях. Во-первых, все шесть функциональных тестов успешно пройдены как для последовательной, так и для параллельной версии. Каждый тест запускался с одним, двумя и четырьмя процессами, и во всех случаях полученные результаты совпали с ожидаемыми.

Особое внимание уделялось граничным случаям. Тест с пустыми строками подтвердил, что алгоритм корректно обрабатывает ситуацию, когда минимальная длина равна нулю. Тест с одинаковыми строками показал, что алгоритм правильно возвращает ноль различий. Тесты с разной длиной строк подтвердили правильность учёта разницы длин.

Для дополнительной проверки была реализована независимая функция вычисления ожидаемого результата в тестовом классе. Эта функция работает по той же логике, что и последовательный алгоритм, но находится в отдельном коде. Совпадение результатов реализации и ожидаемых значений во всех тестах подтверждает корректность обеих версий.

### 7.2 Анализ производительности

Измерения производительности проводились на строках длиной десять миллионов символов. Каждый тест запускался несколько раз, и фиксировалось среднее время выполнения. Результаты представлены в таблицах ниже.

**Режим Task Run:**

| Реализация | Процессов | Время (сек) | Ускорение | Эффективность |
|------------|-----------|-------------|-----------|---------------|
| SEQ        | 1         | 0.00888772  | 1.00      | —             |
| MPI        | 1         | 0.00780446  | 1.13      | 113%          |
| MPI        | 2         | 0.00418256  | 2.12      | 106%          |
| MPI        | 4         | 0.00435498  | 2.16      | 51%           |

**Режим Pipeline:**

| Реализация | Процессов | Время (сек) | Ускорение | Эффективность |
|------------|-----------|-------------|-----------|---------------|
| SEQ        | 1         | 0.00969772  | 1.00      | —             |
| MPI        | 1         | 0.01018988  | 0.95      | 95%           |
| MPI        | 2         | 0.00717252  | 1.35      | 68%           |
| MPI        | 4         | 0.00582360  | 1.67      | 42%           |

**Формулы:**
- Ускорение = Время(SEQ) / Время(MPI)
- Эффективность = (Ускорение / Количество_процессов) × 100%

### 7.3 Интерпретация результатов

**Один процесс**

В режиме task run MPI версия с одним процессом показала ускорение 1.13x, что может объясняться несколькими причинами:
- Компилятор мог применить разные оптимизации для MPI кода
- Возможны эффекты кэша из-за разного порядка доступа к памяти
- MPI библиотека могла использовать оптимизированные низкоуровневые функции

В режиме pipeline наблюдается небольшое замедление (0.95x), что связано с дополнительными накладными расходами на инициализацию MPI окружения.

**Два процесса**

С двумя процессами получены наилучшие результаты — ускорение 2.12x в task режиме и 1.35x в pipeline. Суперлинейное ускорение в task режиме (эффективность 106%) можно объяснить эффектами кэша процессора: когда каждый процесс обрабатывает 5 миллионов символов вместо 10, данные лучше помещаются в L2/L3 кэш, что снижает количество обращений к оперативной памяти, которая является медленной.

**Четыре процесса**

При переходе к четырём процессам эффективность в task режиме упала до 51%, а в pipeline до 42%. Это связано с:
- Увеличением накладных расходов на коммуникацию (больше процессов → больше времени на синхронизацию)
- Возможной конкуренцией за ресурсы, если в системе меньше физических ядер (у меня их всего 2)
- Законом Амдала — есть последовательная часть (добавление разницы длин), которая не параллелится

Тем не менее, ускорение 2.16x в task режиме и 1.67x в pipeline — это хорошие результаты, показывающие, что алгоритм эффективно масштабируется.

**Выводы по производительности**

Параллельная реализация показала свою эффективность на больших объёмах данных. Для 10 миллионов символов получено ускорение до 2.12 раз. Оптимальным оказалось использование двух процессов — дальнейшее увеличение даёт меньший прирост из-за накладных расходов.

Замечу, что в более ранних экспериментах с меньшими строками (10 тысяч символов) MPI версия была медленнее последовательной из-за влияния коммуникационных расходов, которые перевешивали преимущества от параллелизации. Это подтверждает важность выбора правильного размера задачи для эффективного параллелизма.

## 8. Заключение

В ходе выполнения работы была разработана параллельная реализация алгоритма подсчёта несовпадающих символов между двумя строками с использованием технологии MPI. Последовательная версия послужила эталоном корректности и базой для сравнения производительности.

Основные достижения работы включают корректную реализацию распределённого алгоритма с использованием коллективных операций `MPI_Reduce` и `MPI_Bcast`. Все функциональные тесты пройдены успешно при различном количестве процессов, что подтверждает надёжность реализации. Проведён анализ производительности, показавший зависимость эффективности параллелизма от размера задачи.

Полученные результаты демонстрируют, что параллельная обработка данных даёт существенный выигрыш в производительности при условии, что объём вычислений достаточно велик. Для малых задач накладные расходы на коммуникацию могут превысить выигрыш от параллелизма, но с ростом размера данных эффективность увеличивается.

Ограничениями текущей реализации являются необходимость дублирования входных данных на всех процессах и статическое распределение работы. Для очень больших строк, не помещающихся в память одного узла, потребовалась бы схема распределённого хранения с пересылкой только необходимых фрагментов данных.

Возможные направления улучшения включают реализацию адаптивного распределения работы, когда процессы динамически запрашивают новые порции данных по мере завершения текущих. Это могло бы улучшить баланс нагрузки при неравномерном распределении сложности вычислений. Также интересно было бы в будущем реализовать гибридный подход с использованием MPI для межузловой коммуникации и OpenMP для внутриузлового параллелизма.

Работа над проектом позволила глубоко изучить принципы распределённых вычислений и освоить базовые и коллективные операции MPI.

## 9. Список использованных источников

1. Лекции по курсу "Parallel Programming 2025-2026". https://disk.yandex.ru/d/NvHFyhOJCQU65w
2. MPI Forum. MPI: A Message-Passing Interface Standard Version 4.0. https://www.mpi-forum.org/docs/
3. Документация по курсу «Параллельное программирование». https://learning-process.github.io/parallel_programming_course/ru/common_information/report.html
4. Введение в параллельное программирование (OpenMP и MPI). https://stepik.org/course/115024/promo
5. MPI для начинающих. https://parallel.uran.ru/node/182

## Приложение

### Ключевые фрагменты кода

**Последовательная версия:**
```cpp
const auto &input = GetInput();
const std::string &str1 = input.first;
const std::string &str2 = input.second;

size_t result = 0;
size_t min_len = std::min(str1.size(), str2.size());

for (size_t i = 0; i < min_len; ++i) {
  if (str1[i] != str2[i]) {
    ++result;
  }
}

size_t len1 = str1.size();
size_t len2 = str2.size();
result += (len1 > len2) ? (len1 - len2) : (len2 - len1);

GetOutput() = result;
```

**Параллельная версия:**
```cpp
int rank = 0;
int size = 0;

MPI_Comm_rank(MPI_COMM_WORLD, &rank);
MPI_Comm_size(MPI_COMM_WORLD, &size);

if (size <= 0) {
  return false;
}

const auto &input = GetInput();
const std::string &str1 = input.first;
const std::string &str2 = input.second;

int min_len = static_cast<int>(std::min(str1.size(), str2.size()));
size_t len1 = str1.size();
size_t len2 = str2.size();
size_t length_diff = (len1 > len2) ? (len1 - len2) : (len2 - len1);

size_t local_count = 0;

if (min_len > 0) {
  size_t els_per_process = (min_len + size - 1) / size;
  size_t start = rank * els_per_process;
  size_t end = std::min<size_t>(start + els_per_process, min_len);

  for (size_t i = start; i < end; ++i) {
    if (str1[i] != str2[i]) {
      ++local_count;
    }
  }
}

size_t total_count = 0;
MPI_Reduce(&local_count, &total_count, 1, MPI_UNSIGNED_LONG, 
           MPI_SUM, 0, MPI_COMM_WORLD);

if (rank == 0) {
  total_count += length_diff;
  GetOutput() = total_count;
}

size_t result = (rank == 0) ? GetOutput() : 0;
MPI_Bcast(&result, 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);

if (rank != 0) {
  GetOutput() = result;
}
```
