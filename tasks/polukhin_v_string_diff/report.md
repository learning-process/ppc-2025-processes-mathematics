Первая задача из задач параллелизма на процессах

# Подсчет числа несовпадающих символов двух строк



\- Студент: Полухин Владислав Валерьевич, группа 3823Б1ПМоп3

\- Технология: MPI

\- Вариант: 27



\## 1. Введение



Сравнение строк является одной из базовых операций в программировании, которая находит применение в самых разных областях от обработки текстов до анализа ДНК. Эта работа заключается в разработке и анализе MPI реализации алгоритма подсчёта несовпадающих символов между двумя строками.



Цель работы состоит в том, чтобы закрепить на практике принципы параллельных вычислений на кластерных системах, освоить базовые коллективные операции MPI и провести анализ производительности параллельного алгоритма в сравнении с последовательной версией реализации. Также вызывает интерес исследовать масштабируемость решения при увеличении числа вычислительных процессов.



\## 2. Постановка задачи



Постановка задачи выглядит следующим образом: на вход алгоритму подаются две строки произвольной длины, которые могут содержать любые печатные символы из таблицы ASCII; требуется вычислить общее количество позиций, в которых строки различаются.



Алгоритм должен учитывать два типа различий. 
Первый тип - это посимвольные несовпадения на соответствующих позициях строк до конца более короткой из них.
Второй тип - это разница в длинах строк, поскольку в более длинной строке есть некоторое количество символов(уже не важно, каких именно), которым ничего не соответствует в более короткой.



Результат можно выразить как сумму двух компонент. 
Если обозначить длины строк как len1 и len2, то результат равен количеству позиций i от нуля до min(len1, len2), где str1\[i] не равно str2\[i], плюс значение разности max(len1, len2) - min(len1, len2).



Приведу примеры. 
Строки "abc" и "abc" дают результат 0, так как они полностью идентичны. 
Строки "abc" и "abd" дают результат 1, поскольку различаются только str1\[2] и str2\[2]. 
Строки "hello" и "hallo" также дают 1, так как различаются только str1\[1] и str2\[1]. 
Строки "abc" и "abcd" дают результат 1 из-за разницы в длине на один символ.



\## 3. Описание алгоритма



Последовательная версия алгоритма. 
Сначала определяется минимальная длина из двух строк, что позволяет избежать выхода за границы массива при сравнении. Затем инициализируем счётчик различий нулем.



Основной цикл проходит по всем позициям от нуля до min(len1, len2) - 1. На каждой итерации происходит сравнение символов на текущей позиции в обеих строках. Если символы не совпадают, увеличиваем счетчик на единицу.



После завершения цикла к счётчику добавляется разность длин строк. Финальное значение счётчика и является результатом работы алгоритма.





Параллельная версия алгоритма.
Основная идея заключается в том, чтобы мы разделяем диапазон индексов строк для сравнения между несколькими вычислительными процессами количества n, получая n более мелких диапазонов, каждый из которых независимо обрабатывает свою часть и подсчитывает локальные различия.


Логика подсчета различий в каждом из процессов аналогично описанной выше(как в последовательной версии), т.е. тоже проходим в каждом из процессов по позициям доставшегося диапазона и сравниваем символы на текущей позиции в обеих строках; если символы не совпадают, увеличиваем счетчик на единицу.

Дальше собираем результаты подсчета со всех процессов и прибавляем разницу длин строк.





\## 4. Схема распараллеливания



Распределение работы осуществляется на основе номера процесса. Для каждого процесса вычисляется начальный и конечный индексы его рабочего диапазона. Формула для вычисления размера порции данных использует округление вверх, чтобы гарантировать покрытие всех элементов. Конкретно, количество символов на процесс вычисляется как (минимальная\_длина + количество\_процессов - 1), делённое нацело на количество\_процессов.



Начальный индекс для процесса с номером rank вычисляется как rank умноженный на количество символов на процесс. Конечный индекс это либо начальный плюс количество символов на процесс, либо минимальная длина строки, в зависимости от того, что меньше. Такая схема гарантирует то, что последний процесс обработает все оставшиеся элементы, даже если их количество меньше стандартного количества символов на процесс.



После локальных вычислений необходимо собрать результаты со всех процессов. 
Для этого используется коллективная операция MPI\_Reduce с функцией суммирования. Эта операция собирает локальные счётчики со всех процессов и вычисляет их сумму, помещая результат на корневом процессе с номером ноль.



Корневой процесс добавляет к полученной сумме разницу длин строк. После этого финальный результат необходимо распространить на все процессы, поскольку тестовая система проверяет корректность результата на каждом процессе. Поэтому мы пользуемся коллективной операцией MPI\_Bcast, которая рассылает данные от корневого процесса всем остальным.



Топология коммуникаций представляет собой схему <мастер-рабочие> с использованием только коллективных операций. Это, кстати, гарантирует отсутствие взаимных блокировок и обеспечивает эффективную синхронизацию всех процессов.



\## 5. Детали реализации



Код организован в виде модульной структуры согласно требованиям курса. В папке common определены общие типы данных через псевдонимы типов. Входной тип InType это std::pair из двух строк, выходной тип OutType это size\_t для хранения количества различий. Также определён TestType как std::tuple из двух строк для параметризации тестов.



Последовательная версия реализована в отдельной папке seq. Класс StringDiffTaskSEQ наследуется от базового класса Task и переопределяет четыре обязательных метода: ValidationImpl, PreProcessingImpl, RunImpl и PostProcessingImpl. В данной задаче только RunImpl содержит содержательную логику, остальные методы возвращают true, так как специальной валидации или пред и постобработки не требуется.



Параллельная версия находится в папке mpi. Класс StringDiffTaskMPI имеет аналогичную структуру, но метод RunImpl содержит вызовы функций MPI для распределённых вычислений.



В реализации MPI версии особое внимание уделено обработке граничных случаев. Если минимальная длина строк равна нулю, цикл сравнения не выполняется, и все процессы передают в Reduce нулевые локальные счётчики. Если количество символов меньше числа процессов, некоторые процессы получают пустые диапазоны, но это корректно обрабатывается благодаря тому, что всегда start <= end.



Использование коллективных операций вместо точечных коммуникаций существенно упрощает код и повышает его надёжность. MPI\_Reduce автоматически собирает данные со всех процессов, используя внутренне оптимизированный алгоритм, который может работать быстрее, чем последовательный сбор через циклы Send и Recv. Аналогично MPI\_Bcast эффективно распространяет данные, используя древовидную схему коммуникаций.



Память используется эффективно: каждый процесс хранит только копии входных строк и небольшое количество целочисленных переменных для вычислений. Дублирование строк на всех процессах допустимо для данного размера задачи, но при работе с очень большими данными потребовалась бы более сложная схема распределённого хранения.



\## 6. Экспериментальная установка



Все эксперименты проводились на персональном компьютере со следующими характеристиками. 
Процессор: 11th Gen Intel(R) Core(TM) i3-1115G4 @ 3.00GHz (3.00 GHz) с 2 физическими ядрами и поддержкой Hyper-Threading, что даёт 4 логических потока. 

Оперативная память составляет 8,00 ГБ типа DDR4. 

Операционная система: Windows 11 64-разрядная.



В качестве компилятора использовался Microsoft Visual C++ 17.14. 

Для поддержки MPI установлена библиотека Microsoft MPI.



Сборка проекта осуществлялась через систему CMake 3 с использованием типа сборки Release, что активирует максимальные оптимизации компилятора.



Для запуска MPI процессов использовалась утилита mpiexec, входящая в состав Microsoft MPI. Переменная окружения PPC\_NUM\_PROC устанавливалась для контроля количества запускаемых процессов. Тестирование проводилось с одним, двумя и четырьмя процессами для анализа масштабируемости.



Тестовые данные для проверки производительности представляли собой строки длиной десять миллионов символов. Первая строка полностью заполнена символом 'a', вторая также заполнена символом 'a', но каждый десятый символ заменён на 'b'. Это даёт ровно один миллион различий, что позволяет легко проверить корректность результата. Такой размер данных выбран, чтобы время вычислений было достаточно большим для точных измерений и чтобы проявился эффект параллелизации.



Для функциональных тестов использовались шесть тестовых случаев, покрывающих различные сценарии: идентичные строки, одно различие, несколько различий, разная длина и пустые строки. Все функциональные тесты проверялись с одним, двумя и четырьмя процессами, чтобы убедиться в корректности работы при любом уровне параллелизма.



\## 7. Результаты и обсуждение



\### 7.1 Проверка корректности



Корректность реализации проверялась на нескольких уровнях. Во первых, все шесть функциональных тестов успешно пройдены как для последовательной, так и для параллельной версии. Каждый тест запускался с одним, двумя и четырьмя процессами, и во всех случаях полученные результаты совпали с ожидаемыми.



Особое внимание уделялось граничным случаям. Тест с пустыми строками подтвердил, что алгоритм корректно обрабатывает ситуацию, когда минимальная длина равна нулю. Тест с одинаковыми строками показал, что алгоритм правильно возвращает ноль различий. Тесты с разной длиной строк подтвердили правильность учёта разницы длин.



Для дополнительной проверки была реализована независимая функция вычисления ожидаемого результата в тестовом классе. Эта функция работает по той же логике, что и последовательный алгоритм, но находится в отдельном коде. Совпадение результатов реализации и ожидаемых значений во всех тестах подтверждает корректность обеих версий.



\### 7.2 Анализ производительности



Измерения производительности проводились на строках длиной десять миллионов символов. Каждый тест запускался несколько раз, и фиксировалось среднее время выполнения. Результаты представлены в таблицах ниже.



Режим Task Run:



&nbsp; Реализация   Процессов   Время (сек)   Ускорение  Эффективность 

&nbsp;                                                               

&nbsp;    SEQ            1        0.008879      1.00       	 —             

&nbsp;    MPI            1        0.006339      1.40       	140%          

&nbsp;    MPI            2        0.003863      2.30       	115%          

&nbsp;    MPI            4        0.004120      2.16       	54%           



Режим Pipeline:



Реализация   Процессов   Время (сек)   Ускорение  Эффективность

 

    SEQ            1        0.008691     1.00       	—

    MPI            1        0.006218     1.40         140%

    MPI            2        0.004601     1.89         94%

    MPI            4        0.004578     1.90         47%





Ускорение = Время(SEQ) / Время(MPI)  

Эффективность = Ускорение / Количество\_процессов × 100%



Ускорение вычисляется как отношение времени последовательной версии ко времени параллельной. Эффективность это ускорение делённое на количество процессов, выраженное в процентах.



\### 7.3 Интерпретация результатов

Один процесс



Неожиданно обнаружил, что MPI версия с одним процессом работает быстрее последовательной (ускорение 1.4x). Это может объясняться несколькими причинами:

\- Компилятор мог применить разные оптимизации для MPI кода

\- Возможны эффекты кэша из-за разного порядка доступа к памяти

\- MPI библиотека могла использовать оптимизированные низкоуровневые функции




Два процесса



С двумя процессами получил наилучшие результаты — ускорение 2.3x в task режиме и почти 2x в pipeline. Суперлинейное ускорение (эффективность >100%) можно объяснить эффектами кэша процессора: когда каждый процесс обрабатывает 5 миллионов символов вместо 10, данные лучше помещаются в L2/L3 кэш, что снижает количество обращений к оперативной памяти, которая является медленной. Также могла сказаться разница в оптимизациях компилятора для разных версий кода.



Четыре процесса



При переходе к четырём процессам эффективность упала примерно до 50%. Это связано с:

&nbsp; Увеличением накладных расходов на коммуникацию (больше процессов -> больше времени на синхронизацию)

&nbsp; Возможной конкуренцией за ресурсы, если в системе меньше физических ядер(у меня их меньше, всего 2)

&nbsp; Законом Амдала — есть последовательная часть (добавление разницы длин), которая не параллелится



Тем не менее, ускорение в 2.16x на четырёх процессах — это отличный результат, показывающий, что алгоритм можно эффективно масштабировать.





Выводы по производительности:



Параллельная реализация показала свою эффективность на больших объёмах данных. Для 10 миллионов символов получил ускорение до 2.3 раз. Оптимальным оказалось использование двух процессов — дальнейшее увеличение даёт меньший прирост из-за накладных расходов.



Замечу, что в более ранних экспериментах с меньшими строками (10 тысяч символов) MPI версия была медленнее последовательной из-за влияния коммуникационных расходов, которые перевешивали приобретения от параллелизации. Это подтверждает важность выбора правильного размера задачи для эффективного параллелизма.



\## 8. Заключение



В ходе выполнения работы была разработана параллельная реализация алгоритма подсчёта несовпадающих символов между двумя строками с использованием технологии MPI. Последовательная версия послужила эталоном корректности и базой для сравнения производительности.



Основные достижения работы включают корректную реализацию распределённого алгоритма с использованием коллективных операций MPI\_Reduce и MPI\_Bcast. Все функциональные тесты пройдены успешно при различном количестве процессов, что подтверждает надёжность реализации. Проведён анализ производительности, показавший зависимость эффективности параллелизма от размера задачи.



Полученные результаты демонстрируют, что параллельная обработка данных даёт существенный выигрыш в производительности при условии, что объём вычислений достаточно велик. Для малых задач накладные расходы на коммуникацию могут превысить выигрыш от параллелизма, но с ростом размера данных эффективность увеличивается.



Ограничениями текущей реализации являются необходимость дублирования входных данных на всех процессах и статическое распределение работы. Для очень больших строк, не помещающихся в память одного узла, потребовалась бы схема распределённого хранения с пересылкой только необходимых фрагментов данных.



Возможные направления улучшения включают реализацию адаптивного распределения работы, когда процессы динамически запрашивают новые порции данных по мере завершения текущих. Это могло бы улучшить баланс нагрузки при неравномерном распределении сложности вычислений. Также интересно было бы в будущем реализовать гибридный подход с использованием MPI для межузловой коммуникации и OpenMP для внутриузлового параллелизма.



Работа над проектом позволила глубоко изучить принципы распределённых вычислений и освоить базовые и коллективные операции MPI.



\## 9. Список использованных источников



1\. MPI Forum. MPI: A Message-Passing Interface Standard Version 4.0. https://www.mpi-forum.org/docs/.



2\. Документация по курсу «Параллельное программирование»
https://learning-process.github.io/parallel\_programming\_course/ru/common\_information/report.html.



3\. Введение в параллельное программирование (OpenMP и MPI)
https://stepik.org/course/115024/promo



4\. MPI для начинающих
https://parallel.uran.ru/node/182







Приложение. Ключевые фрагменты кода


Сравнение строк в реализации последовательного алгоритма


const auto\& input = GetInput();

const std::string\& str1 = input.first;

const std::string\& str2 = input.second;



size\_t result = 0;

size\_t min\_len = std::min(str1.size(), str2.size());



for (size\_t i = 0; i < min\_len; ++i) {

&nbsp; if (str1\[i] != str2\[i]) {

&nbsp;   ++result;

&nbsp; }

}



result += std::abs(static\_cast<int>(str1.size()) - static\_cast<int>(str2.size()));



GetOutput() = result;






Сравнение строк в реализации параллельного алгоритма


int rank, size;

MPI\_Comm\_rank(MPI\_COMM\_WORLD, \&rank);

MPI\_Comm\_size(MPI\_COMM\_WORLD, \&size);



const auto\& input = GetInput();

const std::string\& str1 = input.first;

const std::string\& str2 = input.second;



int minLen = static\_cast<int>(std::min(str1.size(), str2.size()));

int lengthDiff = static\_cast<int>(std::abs(static\_cast<int>(str1.size()) - static\_cast<int>(str2.size())));



int localCount = 0;



if (minLen > 0) {

&nbsp; int calcNum = (minLen + size - 1) / size;

&nbsp; int start = rank \* calcNum;

&nbsp; int end = std::min(start + calcNum, minLen);



&nbsp; for (int i = start; i < end; ++i) {

&nbsp;   if (str1\[i] != str2\[i]) {

&nbsp;     ++localCount;

&nbsp;   }

&nbsp; }

}



int totalCount = 0;

MPI\_Reduce(\&localCount, \&totalCount, 1, MPI\_INT, MPI\_SUM, 0, MPI\_COMM\_WORLD);



if (rank == 0) {

&nbsp; totalCount += lengthDiff;

&nbsp; GetOutput() = static\_cast<size\_t>(totalCount);

}



size\_t result = (rank == 0) ? GetOutput() : 0;

MPI\_Bcast(\&result, 1, MPI\_UNSIGNED\_LONG\_LONG, 0, MPI\_COMM\_WORLD);



if (rank != 0) {

&nbsp; GetOutput() = result;

}

