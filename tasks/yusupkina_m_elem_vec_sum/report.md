# <Сумма элементов вектора>

- Студент: <Юсупкина Маргарита Альбертовна>, группа <3823Б1ПМоп3>
- Технология: <SEQ | MPI >
- Вариант: <1>

## 1. Введение 
- Мотивация: Исследовать эффективность распараллеливания простой вычислительной задачи через MPI.

- Контекст проблемы: Операция суммирования вектора имеет низкую арифметическую сложность, что может привести к преобладанию коммуникационных затрат над вычислительными.

- Ожидаемый результат: MPI-версия может демонстрировать низкую эффективность из-за значительных расходов на распределение данных.


## 2. Постановка задачи
**Задача:** реализация последовательной (SEQ) и параллельной (MPI) версий вычесления суммы вектора.

- Вход: Вектор целых чисел произвольного размера
- Выход: Сумма всех элементов вектора 

**Ограничения** 
- Вектор может быть пустым 
- Поддерживаются положительные, отрицательные целые числа и нули
- Размер вектора может быть меньше количества процессов


## 3. Описание алгоритма (Последовательного)
Последовательный алгоритм проходит по вектору и считает сумму его элементы с помощью std::accumulate.

## 4. Схема распараллеливания
- For MPI: data distribution, communication pattern/topology, rank roles.
**Проверка особого случая** - если процессов больше чем элементов, работает только процесс 0

**Распределение данных**
- Вектор делится на части между процессами с учетом остатка
- Базовый размер вектора на каждый процесс: `base_size = vec_size / count`
- Если есть остаток, он распределяется по единице первым `remainder` процессам
- Каждый процесс получает `cur_size = base_size + (rank < remainder ? 1 : 0)`

**Коммуникационная схема**
- каждый процесс узнает свой rank и общее количество процессов
- на основании своего номера, общего числа процессов и размера входного вектора определяет границы своих вычислений
- каждый процесс копирует свою часть данных
- вычисление частичной суммы
- объединение всех частичных сумм через MPI_Allreduce 


## 5. Детали реализации
**Струкрута кода**
yusupkina_m_elem_vec_sum/
├── common/
│   └── include/
│       └── common.hpp - Общие типы данных
├── seq/
│   ├── include/
│      └── ops_seq.hpp - Заголовочный файл последовательной версии
│   └── src/
│       └── ops_seq.cpp - Реализация последовательной версии
├── mpi/
│   ├── include/
│       └── ops_mpi.hpp - Заголовочный файл MPI версии
│   └── src/
│       └── ops_mpi.cpp - Реализация MPI версии
└── tests/
    ├── functional/
    |   └── main.cpp - Функциональные тесты
    └── performance/
        └── main.cpp - Тесты производительности

**Ключевые классы**
- `YusupkinaMElemVecSumMPI` — описание параллельного алгоритма
- `YusupkinaMElemVecSumSEQ` — описание последовательного алгоритма
- Определение пространства имен `yusupkina_m_elem_vec_sum` с обозначением используемых типов данных

**Ключевые функции**
- `YusupkinaMElemVecSumMPI(const InType &in)` - конструктор
- `ValidationImpl()` - проверка входных данных
- `PreProcessingImpl()` - предварительная обработка данных
- `RunImpl()` - основная логика вычислений
- `PostProcessingImpl()` - постобработка результатов

**Общее использование памяти программой** — `O(N)`.
## 6. Окружение
- Hardware/OS: AMD Ryzen 7 8845H, 8 ядер, 16 потоков, 32GB RAM, Win11 Pro
- Toolchain: gcc, version 13.3.0, Release
- Environment: PPC_NUM_PROC: 1, 4, 8, Docker container with AMD-V virtualization
- Data: 
    - Functional tests- тесты с заранее заданным размером и содержанием вектора
    - Performance tests- вектор на 10000000 элементов заполняемы последовательно от 0 с шагом 1

## 7. Экспериментальные результаты

### 7.1 Корректность
**Корректность проверена с помощью функциональных тестов, охватывающих:**
- пустые векторы, один элемент, малые размеры
- нули, отрицательные числа, смешанные знаки, однородные значения
- различные размеры векторов для проверки распределения между процессами
- сравнение с известными результатами 

### 7.2 Производительность 

**Для 10 миллионов элементов:**
| Режим | Процессы | Время, с | Ускорение | Эффективность |
|-------|----------|----------|-----------|---------------|
| seq   | 1        | 0.00110  | 1.00      | N/A           |
| mpi   | 4        | 0.00352  | 0.313     | 7.8%          |
| mpi   | 8        | 0.00391  | 0.281     | 3.5%          |

На 10 млн элементов MPI показывает лучшую эффективность (3.5-7.8%), хотя все еще уступает SEQ версии

**Для 100 миллионов элементов:**
| Режим | Процессы | Время, с | Ускорение | Эффективность |
|-------|----------|----------|-----------|---------------|
| seq   | 1        | 0.01055  | 1.00      | N/A           |
| mpi   | 4        | 0.09845  | 0.107     | 2.7%          |
| mpi   | 8        | 0.08329  | 0.127     | 1.6%          |

На 100 млн элементов эффективность падает (1.6-2.7%) из-за роста затрат на коммуникацию

## 8. Выводы
- Обе версии (SEQ и MPI) работают правильно на всех тестовых случаях
- Увеличение количества процессов ухудшает производительность
- Простое суммирование O(N) не компенсирует накладные расходы параллелизации

MPI-реализация суммирования вектора демонстрирует неэффективность, подтверждая теоретические ожидания о непригодности простых операций для параллельных вычислений.

## 9. References
1. Документация по курсу «Параллельное программирование» - URL: https://learning-process.github.io/parallel_programming_course/ru/
2. Курс лекций по параллельному программированию -URL: https://source.unn.ru

