## Министерство науки и высшего образования Российской Федерации  
## Федеральное государственное автономное образовательное учреждение высшего образования  
## **«Национальный исследовательский Нижегородский государственный университет им. Н.И. Лобачевского»**  
### Институт информационных технологий, математики и механики  

### **Направление подготовки:** «Прикладная математика и информатика»  

---

# Отчёт  
### По задаче  
**Передача от всех одному и рассылка (allreduce)**  
**Вариант №3**

**Выполнил:**  
Студент группы 3823Б1ПМоп3  
**Загрядсков М.А.**

**Преподаватель:**  
доцент **Сысоев А.В.**

**Нижний Новгород, 2025**

---

## Введение

В данной работе реализован и исследован алгоритм Allreduce, который осуществляет редукцию данных со всех процессов согласно указаной операции и рассылку результата всем процессам. Задача решается только с использованием функций MPI_Send и MPI_Recv технологии MPI (Message Passing Interface). Реализованная функция имеет тот же прототип, что и функция MPI_Allreduce.

---

## Постановка задачи

Необходимо:
1. Реализовать метод Allreduce, используя только функции Send и Recv. Реализованная функция должна иметь тот же прототип, что и функция MPI_Allreduce. Тестовая программа должна выполнять пересылку массива как минимум следующих типов: MPI_INT, MPI_FLOAT, MPI_DOUBLE как минимум для следующих операций: MPI_MAX, MPI_MIN, MPI_SUM. Во всех операциях передача должна выполняться с использованием «дерева» процессов.
2. Перед выполнением метода необходимо реализовать метод, который для заданного вектора, количества элементов для каждого процесса а также указанной операции распределяет входные данные по процессам. Для этого метода разрешено использование прочих функций MPI, таких как MPI_Bcast и MPI_Scatter

Последовательный алгоритм в таком случае вызывает стандартную функцию MPI_Allreduce, собственная реализация сравнивается со стандартной по времени работы.

---

## Описание схемы параллельного алгоритма

Реализация основана на попарной пересылке данных между процессами и параллельном применении операции редукции к данным. Это позволяет выполнить вместо **k** операций редукции, где k - количество процессов, **ceil(log<sub>2</sub>(k))**, где ceil - округление к ближайшему большему целому числу. При этом, чтобы обеспечить взаимодействие каждого процесса с каждым, текущий партнер выбирается засчёт сложения по модулю 2 номера текущего процесса с числом 2 в степени шаг цикла, где шаг цикла меняется от 0 до **floor(log<sub>2</sub>(k))**, floor - округление к ближайшему меньшему целому числу.

Рассмотрим параллельную реализацию шаг за шагом:

1. Получаем номер процесса через `MPI_Comm_rank`, количество процессов через `MPI_Comm_size`
2. Согласно размеру данных выделяем вспомогательный массив для хранения данных от другого процесса
3. Копируем данные с входного массива в результирующий, ближайшую снизу к количеству процессов степень 2 - **p2**
4. Для процессов, номера которых больше **p2** выполняем передачу массива `MPI_Send` первым **p2** процессам, применяем редукцию
5. В цикле по степеням 2 пока число не превысит **p2** вычисляем партнёра используя побитовый XOR. Выполняем обмен данными, применяем операцию редукции
6. После завершения цикла отправляем результат процессам, номера которых больше **p2**

Таким образом, каждый процесс получит редуцированные данные, количество операций редукции имеет логарифмическую асимптотику относительно количества процессов.

---

## Описание MPI-версии

Программная реализация использует тип входных данных в виде кортежа:  
`std::tuple<std::vector<int>, int, int>`, 
где  
- первый элемент — вектор входных данных,  
- второй — количество данных count для каждого процесса
- третий — номер операции редукции op

Для подготовки данных в функции `PreProcessing` используется `MPI_Bcast` `MPI_Scatter`. Функция редукции ApllyOp является шаблонной относительно типа, что позволяет использовать одну реализацию для нескольких типов данных. 

---

## Результаты экспериментов и подтверждение корректности

Эксперименты проводились на локальной машине.  
Время работы сравнивается с временем работы стандартной реализации.
Параметры тестовых данных:
- размер **50.000.000**.
- элементы типа `int`.
- Операция редукции `MPI_MAX`
- использовался генератор псевдослучайных чисел `mt19937`, также известный известный как **«Вихрь Мерсенна»**.
- Диапазон значений элементов: от -100 до 100.
- при экспериментах выделялось 10.000.000 элементов на процесс (т.е. при увеличении количества процессов общее количество данных росло)


| Версия алгоритма | Время выполнения (с), 2 потока | 4 потока | 
|------------------:|---------------------:|---------------------:|
| Стандартная реализация | 0.03 | 0.05 |
| Собственная реализация | 0.09 | 0.22 |

Таким образом, собственная реализация остает от стандартной в 3-4 раза.

**Подтверждение корректности:**  
Тестирование проводилось для всех реализованных операций редукции. 
Проверка корректности осуществляется засчёт использования стандартного MPI_Allreduce.
Все функциональные тесты и тесты производительности были успешно пройдены на локальной машине.

---

## Выводы из результатов

Собственная реализация показывает замедление примерно в 3 раза по сравнению с стандартной версией при запуске на локальном устройстве, что является допустимым значением.

---

## Заключение

В работе реализована и протестирована реализация MPI_Allreduce для некоторых стандартных типов данных и операций редукции.
Проведённые эксперименты подтвердили корректность данной реализации

---

## Список литературы

1. Документация в формате веб-сайта по реализации **MPICH** стандарта **MPI**: [https://www.mpich.org](https://www.mpich.org)

---

## Приложение

### Параллельная реализация

```cpp
int ZagryadskovMAllreduceMPI::MyAllreduce(const void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype,
                                          MPI_Op op, MPI_Comm comm) {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(comm, &rank);
  MPI_Comm_size(comm, &size);

  int type_size = 0;
  MPI_Type_size(datatype, &type_size);
  std::vector<unsigned char> container_buf(static_cast<size_t>(count) * static_cast<size_t>(type_size));
  void *tempbuf = reinterpret_cast<void *>(container_buf.data());

  memcpy(recvbuf, sendbuf, static_cast<size_t>(count) * static_cast<size_t>(type_size));

  int p2 = 1;
  while (p2 << 1 <= size) {
    p2 <<= 1;
  }

  if (rank >= p2) {
    int partner = rank - p2;

    MPI_Send(recvbuf, count, datatype, partner, 0, comm);
    MPI_Recv(recvbuf, count, datatype, partner, 0, comm, MPI_STATUS_IGNORE);

    return MPI_SUCCESS;
  }

  if (rank + p2 < size) {
    int partner = rank + p2;
    MPI_Recv(tempbuf, count, datatype, partner, 0, comm, MPI_STATUS_IGNORE);
    ApplyOp(recvbuf, tempbuf, count, datatype, op, comm);
  }

  for (int step = 0; (1 << step) < p2; step++) {
    int partner = rank ^ (1 << step);

    MPI_Request request = MPI_REQUEST_NULL;
    MPI_Isend(recvbuf, count, datatype, partner, 0, comm, &request);
    MPI_Recv(tempbuf, count, datatype, partner, 0, comm, MPI_STATUS_IGNORE);
    MPI_Wait(&request, MPI_STATUS_IGNORE);

    ApplyOp(recvbuf, tempbuf, count, datatype, op, comm);
  }

  if (rank + p2 < size) {
    int partner = rank + p2;
    MPI_Send(recvbuf, count, datatype, partner, 0, comm);
  }

  return MPI_SUCCESS;
}
```