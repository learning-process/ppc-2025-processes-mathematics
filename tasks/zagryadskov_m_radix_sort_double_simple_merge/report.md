## Министерство науки и высшего образования Российской Федерации  
## Федеральное государственное автономное образовательное учреждение высшего образования  
## **«Национальный исследовательский Нижегородский государственный университет им. Н.И. Лобачевского»**  
### Институт информационных технологий, математики и механики  

### **Направление подготовки:** «Прикладная математика и информатика»  

---

# Отчёт  
### По задаче  
**Поразрядная сортировка для вещественных чисел (тип double) с простым слиянием**  
**Вариант №20**

**Выполнил:**  
Студент группы 3823Б1ПМоп3  
**Загрядсков М.А.**

**Преподаватель:**  
доцент **Сысоев А.В.**

**Нижний Новгород, 2025**

---

## Введение

В данной работе реализован и исследован алгоритм сортировки чисел в формате double с использованием поразрядной сортировки с простым слиянием. Задача решается в последовательной и в параллельной форме с использованием технологии MPI (Message Passing Interface). Параллельная реализация направлена на сокращение времени вычислений за счёт распределения данных по процессам. Используется LSD (Least Significant Digit) вариант сортировки.

---

## Постановка задачи

Необходимо для заданного массива чисел double отсортировать его в порядке возрастания с использованием поразрядной сортировки (в дальнейшем Radix Sort). При этом нужно реализовать последовательную и параллельную версии алгоритмов  

Требуется:  
1. Реализовать последовательный алгоритм Radix Sort для чисел типа double.  
2. Разработать параллельную реализацию с использованием **MPI**, распределяющую вычисления между несколькими процессами.  
3. Проверить корректность вычислений и провести экспериментальные замеры времени выполнения.  

---

## Описание алгоритма

Поразрядная сортировка основана на сортировке подсчётом, которая использует ограниченность диапазона данных для сортировки без их сравнения. В сортировке подсчётом создается вспомогательный массив из **D+1** нулей, где **D** – разность между максимальным и минимальным значениями диапазона. Допуская, что исходные данные неотрицательны, проходим по исходному массиву и прибавляем единицу к ячейке вспомогательного массива с индексом, равным значению текущего элемента. После полного прохода по исходному массиву формируем отсортированный массив: проходим по вспомогательному массиву, кладем в результирующий массив количество элементов, равных текущему индексу вспомогательного массива, равное значению элемента по данному индексу во вспомогательном массиве.
Для поразрядной сортировки произвольных чисел используем их байтовое представление в системе. Тогда вспомогательный массив будет иметь размер **D = 256** – диапазон значений однобайтового беззнакового целого числа плюс единица. Будем сортировать исходные числа, начиная с наименее значащего байта и заканчивая наиболее значащим (Least Significant Digit тип сортировки), при этом по байтам числа сортируются при помощи сортировки подсчётом. После W итераций, где W – размер исходного типа данных в байтах, массив будет отсортирован. После сортировки массива по одному разрядку важно при составлении промежуточного массива сохранять тот порядок, который имели числа до сортировки (см. [2]). Сложность такого алгоритма при любых исходных данных составляет **O(W*n)**. При сортировке чисел с плавающей запятой неотрицательные числа будут отсортированы в правильном порядке (поскольку в их представлении наименее значащие байты находятся с той же стороны, что и у целых чисел), а отрицательные – в правильном порядке по модулю, то есть в обратном по значению. Необходимо учитывать эти особенности при формировании отсортированного массива при работе с указанными типами данных.

Последовательная версия выполняет следующие шаги:

1. Проверяет корректность входных данных.  
2. Выполняет сортировку подсчётом. Для этого для каждого байта числа:
   - Выполняется сортировка подсчётом.
   - На основе вспомогательного массива строится массив префиксных сумм с 0 по нулевому индексу.
   - Выполняется перегруппировка элементов исходного массива согласно этому массиву с сохранением порядка.
3. После порязрядной сортировки выполняет переупорядочивание чисел, отрицательные числа перемещаются в начало массива в обратном порядке, а положительные числа в конец массива в прямом порядке. 
4. Копирует данные в результирующий массив

---

## Описание схемы параллельного алгоритма

Существует несколько подходов к распараллеливанию данной сортировки, например, засчёт распределения данных процессам на основе старшего байта или распараллеливания сортировки подсчётом. Однако, первый вариант требует знаний о распределении и диапазоне данных в массиве, а второй не так эффективен, поскольку переупорядочивание массива после сортировки подсчётом в таком случае не будет распараллелено. Третий подход заключается в изначальном распределении
Параллельная версия использует распределение массива между процессами, затем каждый из них выполняет последовательный алгоритм поразрядной сортировки после чего производится слияние массивов засчёт дерева процессов.

---

## Описание MPI-версии

Программная реализация использует тип входных данных в виде кортежа:  
`std::tuple<size_t, std::vector<double>>`, 
где  
- первый элемент — количество столбцов `n`,  
- второй — вектор значений матрицы, хранящийся по столбцам.  

MPI-реализация опеределяет количество столбцов для каждого процесса, производит рассылку данных и сбор частичных результатов. Используется синхронизация через `MPI_Barrier`.

---

## Результаты экспериментов и подтверждение корректности

Эксперименты проводились на локальной машине.  
Параметры тестовой матрицы:
- размер **2048 × 4096**.
- элементы типа `double`.
- использовался генератор псевдослучайных чисел `mt19937`, также известный известный как **«Вихрь Мерсенна»**.
- Диапазон значений элементов матрицы: от -10<sup>6</sup> до 10<sup>6</sup>.


| Версия алгоритма | Время выполнения (с) |
|------------------:|---------------------:|
| Последовательная | 0.053 |
| Параллельная (MPI) | 0.019 |

**Подтверждение корректности:**  
Функция тестирования проверяет, что для каждого столбца найденный максимум не меньше всех элементов столбца.  
Все функциональные тесты и тесты производительности были успешно пройдены на локальной машине.

---

## Выводы из результатов

Реализация с использованием MPI показывает ускорение примерно в **2.8 раза** по сравнению с последовательной версией запуске на локальном устройстве.  
Это демонстрирует эффективность параллельного подхода при работе с крупными матрицами.  
При увеличении числа процессов можно ожидать дальнейшего сокращения времени выполнения, однако при малых размерах матриц затраты на коммуникацию могут нивелировать прирост производительности.

---

## Заключение

В работе реализованы и протестированы последовательная и параллельная (MPI) версии алгоритма нахождения максимальных значений по столбцам матрицы.  
Проведённые эксперименты подтвердили корректность и эффективность параллельной реализации.  

---

## Список литературы

1. Документация в формате веб-сайта по реализации **MPICH** стандарта **MPI**: [https://www.mpich.org](https://www.mpich.org)
2. 6.	Д. Э. Кнут, Искусство программирования том 3, 2-е издание, ред. Ю. В. Козаченко,  ISBN 5-8459-0082-1 – стр. 192 - 201.

---

## Приложение

### Параллельная реализация

```cpp
int world_size = 0;
  int world_rank = 0;
  int err_code = 0;
  err_code = MPI_Comm_size(MPI_COMM_WORLD, &world_size);
  if (err_code != MPI_SUCCESS) {
    throw std::runtime_error("MPI_Comm_size failed");
  }
  err_code = MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
  if (err_code != MPI_SUCCESS) {
    throw std::runtime_error("MPI_Comm_rank failed");
  }
  int n = 0;
  const void *mat_data = nullptr;
  int m = 0;
  OutType &res = GetOutput();
  OutType local_res;
  OutType columns;
  std::vector<int> sendcounts(world_size);
  std::vector<int> displs(world_size);
  if (!displs.empty()) {
    displs[0] = 0;
  }

  if (world_rank == 0) {
    n = static_cast<int>(std::get<0>(GetInput()));
    const auto &mat = std::get<1>(GetInput());
    m = static_cast<int>(mat.size()) / n;
    mat_data = reinterpret_cast<const void *>(mat.data());
  }
  err_code = MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);
  if (err_code != MPI_SUCCESS) {
    throw std::runtime_error("MPI_Bcast failed");
  }
  err_code = MPI_Bcast(&m, 1, MPI_INT, 0, MPI_COMM_WORLD);
  if (err_code != MPI_SUCCESS) {
    throw std::runtime_error("MPI_Bcast failed");
  }

  int columns_count = n / world_size;
  using T = double;  // datatype cannot be sent to other processes
  MPI_Datatype datatype = MPI_DOUBLE;

  int i = 0;
  int j = 0;
  int r = 0;
  T tmp = std::numeric_limits<T>::lowest();
  bool tmp_flag = false;

  if (world_rank == 0) {
    res.assign(n, std::numeric_limits<T>::lowest());
  }
  for (r = 0; r < world_size; ++r) {
    sendcounts[r] = (columns_count + static_cast<int>(r < (n % world_size))) * m;
    if (r > 0) {
      displs[r] = displs[r - 1] + sendcounts[r - 1];
    }
  }

  local_res.assign(static_cast<size_t>(sendcounts[world_rank] / m), std::numeric_limits<T>::lowest());
  columns.resize(sendcounts[world_rank]);
  err_code = MPI_Scatterv(mat_data, sendcounts.data(), displs.data(), datatype, columns.data(), sendcounts[world_rank],
                          datatype, 0, MPI_COMM_WORLD);
  if (err_code != MPI_SUCCESS) {
    throw std::runtime_error("MPI_Scatterv failed");
  }
  for (j = 0; std::cmp_less(j, local_res.size()); ++j) {
    for (i = 0; i < m; ++i) {
      tmp = columns[(j * m) + i];
      tmp_flag = tmp > local_res[j];
      local_res[j] = (static_cast<T>(tmp_flag) * tmp) + (static_cast<T>(!tmp_flag) * local_res[j]);
    }
  }

  for (r = 0; r < world_size; ++r) {
    sendcounts[r] /= m;
    if (r > 0) {
      displs[r] = displs[r - 1] + sendcounts[r - 1];
    }
  }

  err_code = MPI_Gatherv(local_res.data(), static_cast<int>(local_res.size()), datatype, res.data(), sendcounts.data(),
                         displs.data(), datatype, 0, MPI_COMM_WORLD);
  if (err_code != MPI_SUCCESS) {
    throw std::runtime_error("MPI_Gatherv failed");
  }
  if (world_rank != 0) {
    res.resize(n);
  }
  // sequential version requires not to call MPI funcs
  err_code = MPI_Bcast(res.data(), static_cast<int>(res.size()), datatype, 0, MPI_COMM_WORLD);
  if (err_code != MPI_SUCCESS) {
    throw std::runtime_error("MPI_Bcast failed");
  }

  bool result = false;
  if (world_rank == 0) {
    result = !res.empty();
  } else {
    result = true;
  }
  err_code = MPI_Barrier(MPI_COMM_WORLD);
  if (err_code != MPI_SUCCESS) {
    throw std::runtime_error("MPI_Barrier failed");
  }
  return result;
```