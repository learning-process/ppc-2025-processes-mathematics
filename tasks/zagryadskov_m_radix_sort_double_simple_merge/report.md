## Министерство науки и высшего образования Российской Федерации  
## Федеральное государственное автономное образовательное учреждение высшего образования  
## **«Национальный исследовательский Нижегородский государственный университет им. Н.И. Лобачевского»**  
### Институт информационных технологий, математики и механики  

### **Направление подготовки:** «Прикладная математика и информатика»  

---

# Отчёт  
### По задаче  
**Поразрядная сортировка для вещественных чисел (тип double) с простым слиянием**  
**Вариант №20**

**Выполнил:**  
Студент группы 3823Б1ПМоп3  
**Загрядсков М.А.**

**Преподаватель:**  
доцент **Сысоев А.В.**

**Нижний Новгород, 2025**

---

## Введение

В данной работе реализован и исследован алгоритм сортировки чисел в формате double с использованием поразрядной сортировки с простым слиянием. Задача решается в последовательной и в параллельной форме с использованием технологии MPI (Message Passing Interface). Параллельная реализация направлена на сокращение времени вычислений за счёт распределения данных по процессам. Используется LSD (Least Significant Digit) вариант сортировки. Последовательная реализация поразрядной сортировки уже была реализована мной на 1 курсе, поэтому основная идея взята из этой реализации, см. [3].

---

## Постановка задачи

Необходимо для заданного массива чисел double отсортировать его в порядке возрастания с использованием поразрядной сортировки (в дальнейшем Radix Sort). При этом нужно реализовать последовательную и параллельную версии алгоритмов  

Требуется:  
1. Реализовать последовательный алгоритм Radix Sort для чисел типа double.  
2. Разработать параллельную реализацию с использованием **MPI**, распределяющую вычисления между несколькими процессами.  
3. Проверить корректность вычислений и провести экспериментальные замеры времени выполнения.  

---

## Описание алгоритма

Поразрядная сортировка основана на сортировке подсчётом, которая использует ограниченность диапазона данных для сортировки без их сравнения. В сортировке подсчётом создается вспомогательный массив из **D+1** нулей, где **D** – разность между максимальным и минимальным значениями диапазона. Допуская, что исходные данные неотрицательны, проходим по исходному массиву и прибавляем единицу к ячейке вспомогательного массива с индексом, равным значению текущего элемента. После полного прохода по исходному массиву формируем отсортированный массив: проходим по вспомогательному массиву, кладем в результирующий массив количество элементов, равных текущему индексу вспомогательного массива, равное значению элемента по данному индексу во вспомогательном массиве.
Для поразрядной сортировки произвольных чисел используем их байтовое представление в системе. Тогда вспомогательный массив будет иметь размер **D = 256** – диапазон значений однобайтового беззнакового целого числа плюс единица. Будем сортировать исходные числа, начиная с наименее значащего байта и заканчивая наиболее значащим (Least Significant Digit тип сортировки), при этом по байтам числа сортируются при помощи сортировки подсчётом. После W итераций, где W – размер исходного типа данных в байтах, массив будет отсортирован. После сортировки массива по одному разрядку важно при составлении промежуточного массива сохранять тот порядок, который имели числа до сортировки (см. [2]). Сложность такого алгоритма при любых исходных данных составляет **O(W*n)**. При сортировке чисел с плавающей запятой неотрицательные числа будут отсортированы в правильном порядке (поскольку в их представлении наименее значащие байты находятся с той же стороны, что и у целых чисел), а отрицательные – в правильном порядке по модулю, то есть в обратном по значению. Необходимо учитывать эти особенности при формировании отсортированного массива при работе с указанными типами данных.

Последовательная версия выполняет следующие шаги:

1. Проверяет корректность входных данных.  
2. Выполняет сортировку подсчётом. Для этого для каждого байта числа:
   - Выполняется сортировка подсчётом.
   - На основе вспомогательного массива строится массив префиксных сумм с 0 по нулевому индексу.
   - Выполняется перегруппировка элементов исходного массива согласно этому массиву с сохранением порядка.
3. После порязрядной сортировки выполняет переупорядочивание чисел, отрицательные числа перемещаются в начало массива в обратном порядке, а положительные числа в конец массива в прямом порядке. 
4. Копирует данные в результирующий массив

---

## Описание схемы параллельного алгоритма

Существует несколько подходов к распараллеливанию данной сортировки, например, засчёт распределения данных процессам на основе старшего байта или распараллеливания сортировки подсчётом. Однако, первый вариант требует знаний о распределении и диапазоне данных в массиве, а второй не так эффективен, поскольку переупорядочивание массива после сортировки подсчётом в таком случае не будет распараллелено. Третий подход заключается в изначальном распределении равного количества данных процессам произвольным образом, выполнение сортировки и слияние отсортированных массивов в один.

Таким образом, параллельная версия использует распределение массива между процессами, затем каждый из них выполняет последовательный алгоритм поразрядной сортировки после чего производится слияние массивов засчёт дерева процессов.

---

## Описание MPI-версии

Программная реализация использует тип входных и выходных данных `std::vector<double>`. MPI_реализация:
1. Определяет количество данных, которое получит каждый из процессов, при этом если количество данных не кратно количеству процессов, то первые `n % world_size` процессов получат по одному элементу, где `n` - количество данных в векторе, `world_size` - количество процессов.
2. Выполняет последовательную сортировку полученных данных на каждом из процессов.
3. Выполняет слияние отсортированных массивов в один засчёт дерева процессов, где каждый раз количество взаимодейстсвующих процессов уменьшается вдвое.

---

## Результаты экспериментов и подтверждение корректности

Эксперименты проводились на локальной машине.  
Параметры тестового вектора:
- размер **10.000.123** элементов.
- элементы типа `double`.
- использовался генератор псевдослучайных чисел `mt19937`, также известный известный как **«Вихрь Мерсенна»**.
- Диапазон значений элементов матрицы: от -10<sup>6</sup> до 10<sup>6</sup>.


| Версия алгоритма | Время выполнения (с), 2 потока | Время выполнения (с), 4 потока |
|------------------:|---------------------:|---------------------:|
| Последовательная | 0.52 | 0.52 |
| Параллельная (MPI) | 0.47 | 0.42 |

**Подтверждение корректности:**  
Функция тестирования выполняет сортировку при помощи алгоритма std::sort и затем сравнивает массивы с точностью до машинного epsilon. При тестировании размеры массивов составляли от **10** до **1.000.000**. Все тесты корректности пройдены успешно.

---

## Выводы из результатов

Реализация с использованием MPI показывает ускорение примерно в **1,24 раза** по сравнению с последовательной версией при запуске на удаленной машине github-actions на **4** процессах.  
Это демонстрирует эффективность параллельного подхода при работе с крупными векторами.  
При увеличении числа процессов можно ожидать дальнейшего сокращения времени выполнения, однако при малых размерах входных данных затраты на коммуникацию и слияние могут нивелировать прирост производительности сортировки.

---

## Заключение

В работе реализованы и протестированы последовательная и параллельная (MPI) версии алгоритма поразрядной сортировки с простым слиянием для типов `double`.
Проведённые эксперименты подтвердили корректность и эффективность параллельной реализации.  

---

## Список литературы

1. Документация в формате веб-сайта по реализации **MPICH** стандарта **MPI**: [https://www.mpich.org](https://www.mpich.org)
2. Д. Э. Кнут, Искусство программирования том 3, 2-е издание, ред. Ю. В. Козаченко,  ISBN 5-8459-0082-1 – стр. 192 - 201.
3. https://github.com/midnsun/mp1-3823B1PM1

---

## Приложение

### Последовательная реализация

```cpp
void ZagryadskovMRadixSortDoubleSimpleMergeSEQ::Foffset(const uint8_t *mas, size_t size, size_t offset,
                                                        std::array<uint64_t, 256> &count) {
  size_t i = 0;
  uint64_t tmp = 0;
  memset(count.data(), 0, (255ULL + 1ULL) * sizeof(uint64_t));
  for (i = 0; i < size * sizeof(double); i += sizeof(double)) {
    count.at(mas[i + offset])++;
  }
  tmp = count[0ULL];
  count[0ULL] = 0ULL;
  for (i = 0ULL; i < 255ULL; i++) {
    std::swap(tmp, count.at(i + 1));
    count.at(i + 1) += count.at(i);
  }
}

void ZagryadskovMRadixSortDoubleSimpleMergeSEQ::RadixSortLSD(double *mas, size_t size) {
  size_t i = 0;
  size_t j = 0;
  size_t k = 0;
  size_t tidy_const = 0;
  uint8_t *pm = nullptr;
  std::array<uint64_t, 256> count{};
  std::vector<double> vec_buf(size);
  double *mas2 = vec_buf.data();
  pm = reinterpret_cast<uint8_t *>(mas);

  for (i = 0ULL; i < sizeof(double); i++) {
    Foffset(pm, size, i, count);
    for (j = 0ULL; j < size; j++) {
      tidy_const = count.at(pm[(j * sizeof(double)) + i]);
      mas2[tidy_const] = mas[j];
      count.at(pm[(j * sizeof(double)) + i])++;
    }
    std::swap(mas, mas2);
    pm = reinterpret_cast<uint8_t *>(mas);
  }

  k = 0ULL;
  if (mas[size - 1ULL] < 0.0) {
    for (i = size; i > 0ULL; i--) {
      if (mas[i - 1] > 0.0) {
        break;
      }
      mas2[k++] = mas[i - 1ULL];
    }

    for (i = 0ULL; i < size; i++) {
      if (mas[i] < 0.0) {
        break;
      }
      mas2[k++] = mas[i];
    }

    memcpy(mas, mas2, size * sizeof(double));
  }
}
```

### Параллельная реализация

```cpp
void ZagryadskovMRadixSortDoubleSimpleMergeMPI::MyMPIMerge(std::vector<double> &data) {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  std::vector<double> tmp;

  for (int step = 1; step < size; step <<= 1) {
    if ((rank % (2 * step)) == step) {
      uint64_t n = data.size();
      MPI_Send(&n, 1, MPI_UINT64_T, rank - step, 0, MPI_COMM_WORLD);
      MPI_Send(data.data(), static_cast<int>(n), MPI_DOUBLE, rank - step, 1, MPI_COMM_WORLD);

      return;
    }

    int partner = rank + step;
    if (partner < size) {
      uint64_t recv_size;
      MPI_Recv(&recv_size, 1, MPI_UINT64_T, partner, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
      tmp.resize(data.size() + recv_size);
      std::vector<double> recvbuf(recv_size);
      MPI_Recv(recvbuf.data(), static_cast<int>(recv_size), MPI_DOUBLE, partner, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
      std::merge(data.begin(), data.end(), recvbuf.begin(), recvbuf.end(), tmp.begin());
      data.swap(tmp);
    }
  }
}

bool ZagryadskovMRadixSortDoubleSimpleMergeMPI::RunImpl() {
  int world_size = 0;
  int world_rank = 0;
  int err_code = 0;
  bool res = true;
  err_code = MPI_Comm_size(MPI_COMM_WORLD, &world_size);
  if (err_code != MPI_SUCCESS) {
    throw std::runtime_error("MPI_Comm_size failed");
  }
  err_code = MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
  if (err_code != MPI_SUCCESS) {
    throw std::runtime_error("MPI_Comm_rank failed");
  }
  size_t data_size = 0;
  auto world_size_st = static_cast<size_t>(world_size);
  std::vector<double> data;
  double *in_data = nullptr;
  std::vector<int> sendcounts(world_size_st);
  std::vector<int> displs(world_size_st);
  if (world_rank == 0) {
    data_size = GetInput().size();
    in_data = GetInput().data();
  }
  err_code = MPI_Bcast(&data_size, 1, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);
  if (err_code != MPI_SUCCESS) {
    throw std::runtime_error("MPI_Bcast failed");
  }
  size_t data_by_process = data_size / world_size_st;
  MPI_Datatype datatype = MPI_DOUBLE;

  for (size_t rk = 0; rk < world_size_st; ++rk) {
    sendcounts[rk] = static_cast<int>(data_by_process + static_cast<size_t>(rk < (data_size % world_size_st)));
    if (rk > 0) {
      displs[rk] = displs[rk - 1] + sendcounts[rk - 1];
    }
  }

  data.resize(sendcounts[world_rank]);
  err_code = MPI_Scatterv(in_data, sendcounts.data(), displs.data(), datatype, data.data(), sendcounts[world_rank],
                          datatype, 0, MPI_COMM_WORLD);
  if (err_code != MPI_SUCCESS) {
    throw std::runtime_error("MPI_Scatterv failed");
  }

  ZagryadskovMRadixSortDoubleSimpleMergeSEQ::RadixSortLSD(data.data(), data.size());
  MyMPIMerge(data);

  if (world_rank == 0) {
    GetOutput() = data;
    res = !GetOutput().empty();
  } else {
    res = true;
  }
  return res;
}
```